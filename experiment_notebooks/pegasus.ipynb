{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import logging\n",
    "import textstat\n",
    "import subprocess\n",
    "import bert_score\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from rouge import Rouge\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "import io_functions\n",
    "\n",
    "from transformers import (\n",
    "    PegasusTokenizer,\n",
    "    PegasusForConditionalGeneration,\n",
    ")\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    logging.info(f'GPU Name: {torch.cuda.get_device_name(0)}')\n",
    "    logging.info(f'CUDA Version: {torch.version.cuda}')\n",
    "    logging.info(f'GPU Memory Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB')\n",
    "    logging.info(f'GPU Memory Reserved: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB')\n",
    "\n",
    "    try:\n",
    "        logging.info('\\nDetailed GPU Info:\\n')\n",
    "        subprocess.run(['nvidia-smi'], check=True)\n",
    "    except FileNotFoundError:\n",
    "        logging.info('nvidia-smi not found. Ensure NVIDIA drivers are installed.')\n",
    "else:\n",
    "    logging.info('No GPU detected. Running on CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_id = '312932093'\n",
    "df = io_functions.load_if_scraped(company_id=company_id)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'human-centered-summarization/financial-summarization-pegasus'\n",
    "\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = tokenizer.model_max_length\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=max_length, chunk_overlap=max_length / 10)\n",
    "\n",
    "\n",
    "def summarize_text(text: str):\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    logging.debug([len(c) for c in chunks])\n",
    "\n",
    "    summaries = []\n",
    "    for chunk in tqdm(chunks):\n",
    "        inputs = tokenizer(chunk, return_tensors='pt', truncation=True, max_length=max_length).to(\n",
    "            device\n",
    "        )\n",
    "        summary_ids = model.generate(**inputs, max_length=max_length / 4, early_stopping=True)\n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        summaries.append(summary)\n",
    "\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_summary(text, target_length=max_length):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    logging.debug(f'token size: {len(tokens)}')\n",
    "\n",
    "    combined_summary = text\n",
    "    while len(tokens) > target_length:\n",
    "        summaries = summarize_text(combined_summary)\n",
    "        combined_summary = ' '.join(summaries)\n",
    "        tokens = tokenizer.tokenize(combined_summary)\n",
    "\n",
    "    return combined_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = []\n",
    "for text in tqdm(df.full_text):\n",
    "    summary = recursive_summary(text=text)\n",
    "    summaries.append(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_column = f'{model_name}-summaries'\n",
    "df[summary_column] = summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_summary(row, model_name, type='baseline'):\n",
    "    text_to_summarize = row.full_text\n",
    "    summary = row[summary_column]\n",
    "    uuid = row.uuid\n",
    "    company_id = row.companyid\n",
    "    company_name = row.companyname\n",
    "\n",
    "    rouge_evaluator = Rouge()\n",
    "    rouge_scores = rouge_evaluator.get_scores(summary, text_to_summarize)\n",
    "\n",
    "    if isinstance(rouge_scores, list):\n",
    "        rouge_scores = rouge_scores[0]\n",
    "\n",
    "    reference_tokens = text_to_summarize.split()\n",
    "    candidate_tokens = summary.split()\n",
    "    bleu_score = sentence_bleu([reference_tokens], candidate_tokens)\n",
    "\n",
    "    P, R, F1 = bert_score.score(\n",
    "        [summary], [text_to_summarize], rescale_with_baseline=True, lang='en'\n",
    "    )\n",
    "\n",
    "    original_len = len(text_to_summarize.split())\n",
    "    summary_len = len(summary.split())\n",
    "    compression_ratio = summary_len / original_len if original_len > 0 else 0\n",
    "\n",
    "    readability = textstat.flesch_reading_ease(summary)\n",
    "\n",
    "    results = {}\n",
    "    results['model_name'] = model_name\n",
    "    results['uuid'] = uuid\n",
    "    results['companyid'] = company_id\n",
    "    results['companyname'] = company_name\n",
    "\n",
    "    for metric, scores in rouge_scores.items():\n",
    "        results[f'{metric}_r'] = scores['r']\n",
    "        results[f'{metric}_p'] = scores['p']\n",
    "        results[f'{metric}_f'] = scores['f']\n",
    "\n",
    "    results['bleu'] = bleu_score\n",
    "    results['bert_precision'] = P.item()\n",
    "    results['bert_recall'] = R.item()\n",
    "    results['bert_f1'] = F1.item()\n",
    "    results['compression_ratio'] = compression_ratio\n",
    "    results['readability'] = readability\n",
    "\n",
    "    return pd.DataFrame([results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = pd.DataFrame()\n",
    "\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "    evaluation_result = evaluate_summary(row, model_name)\n",
    "    evaluation_results = pd.concat([evaluation_results, evaluation_result], ignore_index=True)\n",
    "\n",
    "evaluation_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_filename = Path('..') / 'data' / 'evaluation_results.csv'\n",
    "\n",
    "if os.path.exists(csv_filename):\n",
    "    existing_df = pd.read_csv(csv_filename)\n",
    "    if ((existing_df.model_name == model_name) & (existing_df.companyid == company_id)).any():\n",
    "        logging.info(\n",
    "            f'model {model_name} and {company_id} combination already exists in {csv_filename}. no new row added.'\n",
    "        )\n",
    "        updated_df = existing_df\n",
    "    else:\n",
    "        updated_df = pd.concat([existing_df, evaluation_results], ignore_index=True)\n",
    "        logging.info(f'model {model_name} not found. appending new row to {csv_filename}.')\n",
    "else:\n",
    "    updated_df = evaluation_results\n",
    "    logging.info(f'{csv_filename} not found. creating new file.')\n",
    "\n",
    "updated_df.to_csv(csv_filename, index=False)\n",
    "logging.info(f'results saved to {csv_filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(df):\n",
    "    '''\n",
    "    Visualizes evaluation metrics stored in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing evaluation results with numeric metric columns.\n",
    "\n",
    "    This function produces:\n",
    "    - Histograms for each numeric metric.\n",
    "    - A correlation heatmap of the numeric metrics.\n",
    "    '''\n",
    "    numeric_cols = df.select_dtypes(include='number').columns.tolist()\n",
    "\n",
    "    df[numeric_cols].hist(bins=20, figsize=(15, 10))\n",
    "    plt.suptitle('Histograms of Evaluation Metrics', fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    correlation_matrix = df[numeric_cols].corr()\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "    plt.title('Correlation Heatmap of Evaluation Metrics', fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_results(evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\n",
    "    Path('..') / 'data' / 'summaries' / f'{company_id}_{model_name}.csv'.replace('/', '-'),\n",
    "    sep='\\t',\n",
    "    index=False,\n",
    "    quoting=1,\n",
    "    escapechar='\\\\',\n",
    "    doublequote=True,\n",
    "    quotechar='\"',\n",
    "    lineterminator='\\n',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.full_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[summary_column][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
