{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import bert_score\n",
    "import matplotlib.pyplot as plt\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import textstat\n",
    "from io_functions import get_ollama_version, load_if_scraped, popen, run\n",
    "from llm_parser import DeepSeekAPI\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge import Rouge\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_server_process = popen('ollama serve')\n",
    "\n",
    "ollama_server_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ollama_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process = run('ollama list')\n",
    "\n",
    "print(process.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [line.strip() for line in process.stdout.split('\\n') if line != ''][1:]\n",
    "\n",
    "model_list = [model.split()[0] for model in models]\n",
    "model_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prevent running a non-existing model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_list[0]\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'deepseek-r1:70b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name not in model_list:\n",
    "    run(f'ollama pull {model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(run(f'ollama show {model_name}').stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_model_info(text: str):\n",
    "    embedding_match = re.search(r'embedding length\\s+(\\d+)', text)\n",
    "    context_match = re.search(r'context length\\s+(\\d+)', text)\n",
    "\n",
    "    embedding_length = int(embedding_match.group(1)) if embedding_match else None\n",
    "    context_length = int(context_match.group(1)) if context_match else None\n",
    "\n",
    "    return {'embedding_length': embedding_length, 'context_length': context_length}\n",
    "\n",
    "\n",
    "model_info = extract_model_info(run(f'ollama show {model_name}').stdout)\n",
    "model_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(run('ollama ps').stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_id = '312932093'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_if_scraped(company_id=company_id)\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = DeepSeekAPI(model_name=model_name)\n",
    "\n",
    "print(api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "async def batch_summary_generation(api: DeepSeekAPI, texts) -> list[str]:\n",
    "    summaries = []\n",
    "    times = []\n",
    "\n",
    "    for text in tqdm(texts):\n",
    "        prompt = 'summarize: ' + text\n",
    "\n",
    "        start_time = time.time()\n",
    "        summary = await api.generate(prompt=prompt)\n",
    "        end_time = time.time()\n",
    "        summaries.append(summary)\n",
    "        times.append(end_time - start_time)\n",
    "    return summaries, times\n",
    "\n",
    "\n",
    "summaries, times = await batch_summary_generation(api, df.full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summaries[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_column = f'{model_name}-summaries'\n",
    "df[summary_column] = summaries\n",
    "df['time_spent'] = times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_summary(row, model_name, type='baseline'):\n",
    "    text_to_summarize = row.full_text\n",
    "    summary = row[summary_column]\n",
    "    uuid = row.uuid\n",
    "    company_id = row.companyid\n",
    "    company_name = row.companyname\n",
    "\n",
    "    rouge_evaluator = Rouge()\n",
    "    rouge_scores = rouge_evaluator.get_scores(summary, text_to_summarize)\n",
    "\n",
    "    if isinstance(rouge_scores, list):\n",
    "        rouge_scores = rouge_scores[0]\n",
    "\n",
    "    reference_tokens = text_to_summarize.split()\n",
    "    candidate_tokens = summary.split()\n",
    "    bleu_score = sentence_bleu([reference_tokens], candidate_tokens)\n",
    "\n",
    "    P, R, F1 = bert_score.score(\n",
    "        [summary], [text_to_summarize], rescale_with_baseline=True, lang='en'\n",
    "    )\n",
    "\n",
    "    original_len = len(text_to_summarize.split())\n",
    "    summary_len = len(summary.split())\n",
    "    compression_ratio = summary_len / original_len if original_len > 0 else 0\n",
    "\n",
    "    readability = textstat.flesch_reading_ease(summary)\n",
    "\n",
    "    results = {}\n",
    "    results['model_name'] = model_name\n",
    "    results['uuid'] = uuid\n",
    "    results['companyid'] = company_id\n",
    "    results['companyname'] = company_name\n",
    "    results['time_spent'] = row.time_spent\n",
    "\n",
    "    for metric, scores in rouge_scores.items():\n",
    "        results[f'{metric}_r'] = scores['r']\n",
    "        results[f'{metric}_p'] = scores['p']\n",
    "        results[f'{metric}_f'] = scores['f']\n",
    "\n",
    "    results['bleu'] = bleu_score\n",
    "    results['bert_precision'] = P.item()\n",
    "    results['bert_recall'] = R.item()\n",
    "    results['bert_f1'] = F1.item()\n",
    "    results['compression_ratio'] = compression_ratio\n",
    "    results['readability'] = readability\n",
    "\n",
    "    return pd.DataFrame([results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = pd.DataFrame()\n",
    "\n",
    "for _, row in tqdm(df.iterrows()):\n",
    "    evaluation_result = evaluate_summary(row, model_name)\n",
    "    evaluation_results = pd.concat([evaluation_results, evaluation_result], ignore_index=True)\n",
    "\n",
    "evaluation_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_filename = Path('..') / 'data' / 'evaluation_results.csv'\n",
    "\n",
    "if os.path.exists(csv_filename):\n",
    "    existing_df = pd.read_csv(csv_filename)\n",
    "    if ((existing_df.model_name == model_name) & (existing_df.companyid == company_id)).any():\n",
    "        logging.info(\n",
    "            f'model {model_name} and {company_id} '\n",
    "            f'combination already exists in {csv_filename}. '\n",
    "            f'no new row added.'\n",
    "        )\n",
    "        updated_df = existing_df\n",
    "    else:\n",
    "        updated_df = pd.concat([existing_df, evaluation_results], ignore_index=True)\n",
    "        logging.info(f'model {model_name} not found. appending new row to {csv_filename}.')\n",
    "else:\n",
    "    updated_df = evaluation_results\n",
    "    logging.info(f'{csv_filename} not found. creating new file.')\n",
    "\n",
    "updated_df.to_csv(csv_filename, index=False)\n",
    "logging.info(f'results saved to {csv_filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(df):\n",
    "    '''\n",
    "    Visualizes evaluation metrics stored in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing evaluation results with numeric metric columns.\n",
    "\n",
    "    This function produces:\n",
    "    - Histograms for each numeric metric.\n",
    "    - A correlation heatmap of the numeric metrics.\n",
    "    '''\n",
    "    numeric_cols = df.select_dtypes(include='number').columns.tolist()\n",
    "\n",
    "    df[numeric_cols].hist(bins=20, figsize=(15, 10))\n",
    "    plt.suptitle('Histograms of Evaluation Metrics', fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    correlation_matrix = df[numeric_cols].corr()\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "    plt.title('Correlation Heatmap of Evaluation Metrics', fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_results(evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\n",
    "    Path('..') / 'data' / 'summaries' / f'{company_id}_{model_name}.csv'.replace('/', '-'),\n",
    "    sep='\\t',\n",
    "    index=False,\n",
    "    quoting=1,\n",
    "    escapechar='\\\\',\n",
    "    doublequote=True,\n",
    "    quotechar='\"',\n",
    "    lineterminator='\\n',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.full_text[0][:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[summary_column][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_server_process.terminate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
