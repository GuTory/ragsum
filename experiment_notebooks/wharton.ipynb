{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further information can be obtained on [Wharton's website](https://wrds-www.wharton.upenn.edu/pages/support/manuals-and-overviews/compustat/capital-iq/transcripts/wrds-overview-capitaliq-transcripts-data/#general-description)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wrds\n",
    "import logging\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db: wrds.Connection = wrds.Connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the query\n",
    "Using our WRDS connection, db, we can run a query with some joins and filters.This query retrieves transcript component text as well as transcript and speaker metadata.\n",
    "\n",
    "Three tables are used:\n",
    "\n",
    "- wrds_transcript_detail - transcript metadata\n",
    "- wrds_transcript_person - speaker metadata\n",
    "- ciqtranscriptcomponent - full transcript text\n",
    "The transcript data is filtered to companies with CIQ CompanyId."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_companies_with_id = '''\n",
    "        SELECT DISTINCT d.companyid, d.companyname\n",
    "        FROM ciq.wrds_transcript_detail as d\n",
    "        WHERE date_part('year', mostimportantdateutc) BETWEEN 2023 AND 2024\n",
    "'''\n",
    "\n",
    "companies: pd.DataFrame = db.raw_sql(select_companies_with_id)\n",
    "\n",
    "companies.companyid = companies.companyid.astype(int)\n",
    "\n",
    "companies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies.to_csv(Path(\"..\") / \"data\" / \"companies.csv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_companies(name: str) -> pd.DataFrame:\n",
    "    \"\"\"Filter function for search in companies dataframe\n",
    "    I don't want to reach out to Wharton API with every and each search,\n",
    "    so this function will filter in the pulled dataframe (companies)\n",
    "\n",
    "    Args:\n",
    "        name (str): Company name filter\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: list of companies with their id-s\n",
    "    \"\"\"\n",
    "\n",
    "    return companies[companies.companyname.str.contains(name, case=False, na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_company_id(company: str) -> int | None:\n",
    "    \"\"\"Filtering based on company name and returning the one and only company's id\n",
    "\n",
    "    Args:\n",
    "        company (str): Company name, expected full match\n",
    "\n",
    "    Returns:\n",
    "        int: returned id, None if there's no such company\n",
    "    \"\"\"\n",
    "    filtered: pd.DataFrame = companies[companies.companyname.str.fullmatch(company)]\n",
    "    return filtered.companyid.item() if filtered.shape[0] == 1 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_company_names(ids: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"Filter function for search in companies dataframe based on id\n",
    "    I don't want to reach out to Wharton API with every and each search,\n",
    "    so this function will filter in the pulled dataframe (companies)\n",
    "\n",
    "    Args:\n",
    "        ids (list[str]): Company id filter, all matching companies are returned\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: list of companies with their id-s\n",
    "    \"\"\"\n",
    "\n",
    "    return companies[companies.companyname.isin(ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_name = 'goog'\n",
    "\n",
    "filtered = get_companies(company_name)\n",
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goog: str = 'Google LLC'\n",
    "\n",
    "google_id = get_company_id(goog)\n",
    "google_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chipotle = 'Chipotle Mexican Grill, Inc.'\n",
    "chipotle_id = get_company_id(chipotle)\n",
    "\n",
    "chipotle_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asml = 'ASML Holding N.V.'\n",
    "asml_id = get_company_id(asml)\n",
    "\n",
    "asml_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id_string(ids: list[str]):\n",
    "    company_id_string: str = \"\"\n",
    "    for c in ids:\n",
    "        company_id_string += str(c) + ','\n",
    "\n",
    "    return company_id_string[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_id_list: list = [google_id, chipotle_id, asml_id]\n",
    "\n",
    "company_id_string = get_id_string(company_id_list)\n",
    "company_id_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = f'''\n",
    "            SELECT a.*, b.*, c.componenttext\n",
    "            FROM (\n",
    "                  SELECT * \n",
    "                  FROM ciq.wrds_transcript_detail\n",
    "                  WHERE companyid IN ({google_id})\n",
    "                    AND date_part('year', mostimportantdateutc) BETWEEN 2000 AND 2025\n",
    "                 ) AS a\n",
    "            JOIN ciq.wrds_transcript_person AS b\n",
    "              ON a.transcriptid = b.transcriptid\n",
    "            JOIN ciq.ciqtranscriptcomponent AS c\n",
    "              ON b.transcriptcomponentid = c.transcriptcomponentid\n",
    "            ORDER BY a.transcriptid, b.componentorder;\n",
    "            '''\n",
    "\n",
    "df = db.raw_sql(sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['transcriptpersonname'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts: pd.DataFrame = (\n",
    "    df.groupby([\"companyid\", \"mostimportantdateutc\", \"mostimportanttimeutc\", \"headline\"])\n",
    "    .apply(\n",
    "        lambda group: \"\\n\".join(\n",
    "            f\"{row['speakertypename']}: {row['componenttext']}\" for _, row in group.iterrows()\n",
    "        ),\n",
    "        include_groups=False,\n",
    "    )\n",
    "    .reset_index(name=\"full_text\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts.companyid = transcripts.companyid.astype(int)\n",
    "transcripts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts['word_count'] = transcripts['full_text'].apply(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts['word_count_nltk'] = transcripts['full_text'].apply(\n",
    "    lambda x: len(word_tokenize(str(x)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_transcripts = transcripts.groupby('companyid')\n",
    "\n",
    "for company_id, group in grouped_transcripts:\n",
    "    group.to_csv(\n",
    "        Path(\"..\") / \"data\" / f\"{company_id}.csv\",\n",
    "        sep='\\t',\n",
    "        index=False,\n",
    "        quoting=1,\n",
    "        escapechar='\\\\',\n",
    "        doublequote=True,\n",
    "        quotechar='\"',\n",
    "        lineterminator='\\n',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_google = pd.read_csv(\n",
    "    Path(\"..\") / \"data\" / f\"{google_id}.csv\",\n",
    "    sep='\\t',\n",
    "    quoting=1,\n",
    "    escapechar='\\\\',\n",
    "    doublequote=True,\n",
    "    quotechar='\"',\n",
    ")\n",
    "\n",
    "df_google.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class WhartonCompanyIdSearchCache:\n",
    "    id: int\n",
    "    name: str\n",
    "    df: pd.DataFrame\n",
    "    transcripts: Optional[pd.DataFrame]\n",
    "\n",
    "\n",
    "class WhartonScraper:\n",
    "    \"\"\"\n",
    "    Wrapper Class for Scraping Wharton Transcripts database for a single company\n",
    "    Based on Company name or id.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        connection: wrds.Connection,\n",
    "    ):\n",
    "        self.connection: wrds.Connection = connection\n",
    "        self.company_search_cache: WhartonCompanyIdSearchCache = None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"WhartonScraper(id={self.company_search_cache.id}, name={self.company_search_cache.name})\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"WhartonScraper for company ({self.company_search_cache.name})\"\n",
    "\n",
    "    def pipeline(self, company_id: str) -> None:\n",
    "        \"\"\"Full Pipeline for transcript acquisition from Wharton database, based on `companyid`\n",
    "\n",
    "        Args:\n",
    "            company_id (str): `companyid` to filter by\n",
    "        \"\"\"\n",
    "        self.get_company_by_id(company_id)\n",
    "        self.get_company_transcripts()\n",
    "        self.transcripts_to_csv()\n",
    "\n",
    "    def get_company_by_id(self, company_id: str) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Reaching out to Wharton database to see if `companyid` is present\n",
    "        \"\"\"\n",
    "        if self.company_search_cache and self.company_search_cache.id == company_id:\n",
    "            logging.debug(f\"using cache on company: {company_id}\")\n",
    "            return self.company_search_cache.df\n",
    "\n",
    "        select_company = f\"\"\"\n",
    "            SELECT DISTINCT d.companyid, d.companyname\n",
    "            FROM ciq.wrds_transcript_detail as d\n",
    "            WHERE d.companyid = {company_id}\n",
    "        \"\"\"\n",
    "        df: pd.DataFrame = self.connection.raw_sql(select_company)\n",
    "\n",
    "        if df.shape[0] > 1:\n",
    "            logging.debug(f\"too many results for search: {df.shape[0]}\")\n",
    "            self.company_search_cache = None\n",
    "            return None\n",
    "\n",
    "        self.company_search_cache = WhartonCompanyIdSearchCache(\n",
    "            id=company_id, name=df.companyname[0], df=df, transcripts=None\n",
    "        )\n",
    "        logging.info(f\"information acquired for company: {company_id}\")\n",
    "        return df\n",
    "\n",
    "    def get_company_transcripts(self) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Acquiring company transcripts based on the cached `companyid`\n",
    "        \"\"\"\n",
    "        if not self.company_search_cache:\n",
    "            logging.debug(\"no company cache\")\n",
    "            return None\n",
    "        if self.company_search_cache.transcripts:\n",
    "            logging.debug(\"transcripts already cached\")\n",
    "            return self.company_search_cache.transcripts\n",
    "\n",
    "        query = f\"\"\"\n",
    "            SELECT a.*, b.*, c.componenttext\n",
    "            FROM (\n",
    "                  SELECT * \n",
    "                  FROM ciq.wrds_transcript_detail\n",
    "                  WHERE companyid = {self.company_search_cache.id}\n",
    "                    AND date_part('year', mostimportantdateutc) BETWEEN 2023 AND 2025\n",
    "                 ) AS a\n",
    "            JOIN ciq.wrds_transcript_person AS b\n",
    "              ON a.transcriptid = b.transcriptid\n",
    "            JOIN ciq.ciqtranscriptcomponent AS c\n",
    "              ON b.transcriptcomponentid = c.transcriptcomponentid\n",
    "            ORDER BY a.transcriptid, b.componentorder;\n",
    "            \"\"\"\n",
    "        df = self.connection.raw_sql(query)\n",
    "        df = df.drop([\"transcriptpersonname\"], axis=1)\n",
    "        transcripts: pd.DataFrame = (\n",
    "            df.groupby(\n",
    "                [\n",
    "                    \"companyid\",\n",
    "                    \"mostimportantdateutc\",\n",
    "                    \"mostimportanttimeutc\",\n",
    "                    \"headline\",\n",
    "                ]\n",
    "            )\n",
    "            .apply(\n",
    "                lambda group: \"\\n\".join(\n",
    "                    f\"{row['speakertypename']}: {row['componenttext']}\"\n",
    "                    for _, row in group.iterrows()\n",
    "                ),\n",
    "                include_groups=False,\n",
    "            )\n",
    "            .reset_index(name=\"full_text\")\n",
    "        )\n",
    "        transcripts.companyid = transcripts.companyid.astype(int)\n",
    "        transcripts[\"word_count\"] = transcripts[\"full_text\"].apply(lambda x: len(str(x).split()))\n",
    "        transcripts[\"word_count_nltk\"] = transcripts[\"full_text\"].apply(\n",
    "            lambda x: len(word_tokenize(str(x)))\n",
    "        )\n",
    "\n",
    "        self.company_search_cache.transcripts = transcripts\n",
    "        logging.info(\n",
    "            f\"transcripts acquired for company: {self.company_search_cache.id} with a shape: {transcripts.shape}\"\n",
    "        )\n",
    "        return transcripts\n",
    "\n",
    "    def transcripts_to_csv(self) -> None:\n",
    "        \"\"\"\n",
    "        Writing transcript dataset to file if it is present\n",
    "        \"\"\"\n",
    "        if self.company_search_cache.transcripts is None:\n",
    "            logging.debug(\"no transcript records.\")\n",
    "            return\n",
    "\n",
    "        self.company_search_cache.transcripts.to_csv(\n",
    "            Path(\"..\") / \"data\" / f\"{self.company_search_cache.id}.csv\",\n",
    "            sep=\"\\t\",\n",
    "            index=False,\n",
    "            quoting=1,\n",
    "            escapechar=\"\\\\\",\n",
    "            doublequote=True,\n",
    "            quotechar='\"',\n",
    "            lineterminator=\"\\n\",\n",
    "        )\n",
    "        logging.info(f\"transcripts successfully written to {self.company_search_cache.id}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = WhartonScraper(connection=db)\n",
    "\n",
    "df = scraper.get_company_by_id(google_id)\n",
    "scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.pipeline('31293209')\n",
    "scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class WordCounter:\n",
    "    def __init__(self, text: str, stop_words=None):\n",
    "        self.text = text\n",
    "        self.stop_words = set(stop_words) if stop_words else None\n",
    "        self.word_counts = self._count_words()\n",
    "\n",
    "    def _count_words(self):\n",
    "        words = re.findall(r\"\\b\\w+\\b\", self.text.lower())\n",
    "        if self.stop_words:\n",
    "            words = [word for word in words if word in self.stop_words]\n",
    "        return Counter(words).most_common(36)\n",
    "\n",
    "    def get_count(self, word: str) -> int:\n",
    "        return self.word_counts.get(word.lower(), 0)\n",
    "\n",
    "    def get_all_counts(self) -> dict:\n",
    "        return dict(self.word_counts)\n",
    "\n",
    "    def plot_counts(self):\n",
    "        words, counts = zip(*self.word_counts)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.bar(words, counts, color='skyblue')\n",
    "        plt.xlabel(\"Words\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.title(\"Word Frequency\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = scraper.company_search_cache.transcripts.full_text[0]\n",
    "\n",
    "len(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = WordCounter(example_text, stop_words=stop_words)\n",
    "\n",
    "counter.plot_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_text = example_text.lower()\n",
    "\n",
    "words = word_tokenize(lower_text)\n",
    "\n",
    "words = [word for word in words if word.isalnum()]\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "print(f'original: {len(words)}, filtered: {len(filtered_words)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(example_text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import evaluate\n",
    "\n",
    "filtered_text = \" \".join(filtered_words)\n",
    "\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_word_count = len(filtered_words)\n",
    "target_min = int(source_word_count * 0.10)\n",
    "target_max = int(source_word_count * 0.30)\n",
    "\n",
    "inputs = tokenizer(filtered_text, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "\n",
    "summary_ids = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    num_beams=4,\n",
    "    max_length=target_max,\n",
    "    min_length=target_min,\n",
    "    early_stopping=True,\n",
    ")\n",
    "\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compression_ratio(summary, source):\n",
    "    summary_word_count = len(summary.split())\n",
    "    source_word_count = len(source.split())\n",
    "    return summary_word_count / source_word_count\n",
    "\n",
    "\n",
    "ratio = compression_ratio(summary, filtered_text)\n",
    "print(f\"\\nCompression Ratio related to filted text: {ratio:.2%}\")\n",
    "ratio = compression_ratio(summary, example_text)\n",
    "print(f\"\\nCompression Ratio related to original text: {ratio:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "rouge_scores = rouge.compute(predictions=[summary], references=[example_text])\n",
    "\n",
    "print(\"\\nROUGE Scores:\")\n",
    "for key, value in rouge_scores.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_scores = rouge.compute(predictions=[summary], references=[filtered_text])\n",
    "\n",
    "print(\"\\nROUGE Scores:\")\n",
    "for key, value in rouge_scores.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
