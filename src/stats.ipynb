{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e41346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Correlations between bert_f1 and ROUGE-AVG:\n",
      "  Pearson correlation:  0.4308\n",
      "  Spearman correlation: 0.3238\n",
      "  Kendall's Tau:        0.2209\n",
      "\n",
      "Correlations between meteor and ROUGE-AVG:\n",
      "  Pearson correlation:  0.9461\n",
      "  Spearman correlation: 0.8620\n",
      "  Kendall's Tau:        0.6920\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr, spearmanr, kendalltau\n",
    "from pathlib import Path\n",
    "from scipy.stats import f_oneway, ttest_ind\n",
    "\n",
    "file_path = Path(os.getcwd()).parent / 'data/summarization_evaluation_metrics.csv'\n",
    "\n",
    "existing_df = pd.read_csv(\n",
    "    file_path, \n",
    "    sep='\\t',\n",
    "    quoting=1,\n",
    "    quotechar='\"',\n",
    "    escapechar='\\\\',\n",
    "    doublequote=True,\n",
    "    engine='python',\n",
    ")\n",
    "\n",
    "final_df = existing_df.copy()\n",
    "\n",
    "final_df['ROUGE-AVG'] = final_df[['rouge-1_f', 'rouge-2_f', 'rouge-l_f']].mean(axis=1)\n",
    "\n",
    "metrics = ['bert_f1', 'meteor']\n",
    "\n",
    "for metric in metrics:\n",
    "    valid_data = final_df[[metric, 'ROUGE-AVG']].dropna()\n",
    "    x = valid_data[metric]\n",
    "    y = valid_data['ROUGE-AVG']\n",
    "\n",
    "    pearson_corr, _ = pearsonr(x, y)\n",
    "    spearman_corr, _ = spearmanr(x, y)\n",
    "    kendall_corr, _ = kendalltau(x, y)\n",
    "\n",
    "    print(f\"\\nCorrelations between {metric} and ROUGE-AVG:\")\n",
    "    print(f\"  Pearson correlation:  {pearson_corr:.4f}\")\n",
    "    print(f\"  Spearman correlation: {spearman_corr:.4f}\")\n",
    "    print(f\"  Kendall's Tau:        {kendall_corr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f9b3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = [\n",
    "    group['ROUGE-AVG'].dropna()\n",
    "    for name, group in final_df.groupby('model_name')\n",
    "    if len(group) > 5  # optionally filter out tiny groups\n",
    "]\n",
    "\n",
    "f_stat, p_val = f_oneway(*groups)\n",
    "\n",
    "print(\"ANOVA test across model_name groups (ROUGE-AVG):\")\n",
    "print(f\"  F-statistic: {f_stat:.4f}\")\n",
    "print(f\"  p-value:     {p_val:.4f}\")\n",
    "\n",
    "if p_val < 0.05:\n",
    "    print(\"=> At least one model differs significantly.\")\n",
    "else:\n",
    "    print(\"=> No significant differences among models.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7be9d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairwise t-tests on ROUGE-AVG scores between models:\n",
      "\n",
      "facebook/bart-large-cnn vs google-t5/t5-base:\n",
      "  t-statistic: 3.8370\n",
      "  p-value:     0.0001\n",
      "  => Statistically significant difference\n",
      "\n",
      "facebook/bart-large-cnn vs google/pegasus-x-large:\n",
      "  t-statistic: -1.2734\n",
      "  p-value:     0.2033\n",
      "  => No significant difference\n",
      "\n",
      "facebook/bart-large-cnn vs human-centered-summarization/financial-summarization-pegasus:\n",
      "  t-statistic: 5.0744\n",
      "  p-value:     0.0000\n",
      "  => Statistically significant difference\n",
      "\n",
      "google-t5/t5-base vs google/pegasus-x-large:\n",
      "  t-statistic: -5.2379\n",
      "  p-value:     0.0000\n",
      "  => Statistically significant difference\n",
      "\n",
      "google-t5/t5-base vs human-centered-summarization/financial-summarization-pegasus:\n",
      "  t-statistic: 0.4209\n",
      "  p-value:     0.6740\n",
      "  => No significant difference\n",
      "\n",
      "google/pegasus-x-large vs human-centered-summarization/financial-summarization-pegasus:\n",
      "  t-statistic: 6.8359\n",
      "  p-value:     0.0000\n",
      "  => Statistically significant difference\n",
      "\n"
     ]
    }
   ],
   "source": [
    "checkpoints = [\n",
    "    'facebook/bart-large-cnn',\n",
    "    'google-t5/t5-base',\n",
    "    'google/pegasus-x-large',\n",
    "    'human-centered-summarization/financial-summarization-pegasus',\n",
    "]\n",
    "\n",
    "models = []\n",
    "for checkpoint in checkpoints:\n",
    "    models.append(final_df[final_df['model_name'] == checkpoint]['ROUGE-AVG'].dropna())\n",
    "\n",
    "print(\"Pairwise t-tests on ROUGE-AVG scores between models:\\n\")\n",
    "\n",
    "for i in range(len(checkpoints)):\n",
    "    for j in range(i + 1, len(checkpoints)):\n",
    "        model_1 = models[i]\n",
    "        model_2 = models[j]\n",
    "        name_1 = checkpoints[i]\n",
    "        name_2 = checkpoints[j]\n",
    "\n",
    "        t_stat, p_val = ttest_ind(model_1, model_2, equal_var=False)\n",
    "\n",
    "        print(f\"{name_1} vs {name_2}:\")\n",
    "        print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "        print(f\"  p-value:     {p_val:.4f}\")\n",
    "        if p_val < 0.05:\n",
    "            print(\"  => Statistically significant difference\\n\")\n",
    "        else:\n",
    "            print(\"  => No significant difference\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
