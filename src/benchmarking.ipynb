{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/persistent/ragsum/venv/lib/python3.10/site-packages/_distutils_hack/__init__.py:53: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "from utils import SummarizationPipeline, ModelConfig, LoggingConfig, load_all_available_transcripts, TextChunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 21:57:27,962 - utils.wharton_processor - INFO - 32307\n",
      "2025-04-29 21:57:28,052 - utils.wharton_processor - INFO - Successfully loaded local transcripts for 32307\n",
      "2025-04-29 21:57:28,053 - utils.wharton_processor - INFO - Successfully loaded 32307.csv\n",
      "2025-04-29 21:57:28,054 - utils.wharton_processor - INFO - 126475\n",
      "2025-04-29 21:57:28,075 - utils.wharton_processor - INFO - Successfully loaded local transcripts for 126475\n",
      "2025-04-29 21:57:28,076 - utils.wharton_processor - INFO - Successfully loaded 126475.csv\n",
      "2025-04-29 21:57:28,077 - utils.wharton_processor - INFO - 26446\n",
      "2025-04-29 21:57:28,087 - utils.wharton_processor - INFO - Successfully loaded local transcripts for 26446\n",
      "2025-04-29 21:57:28,088 - utils.wharton_processor - INFO - Successfully loaded 26446.csv\n",
      "2025-04-29 21:57:28,088 - utils.wharton_processor - INFO - 388904\n",
      "2025-04-29 21:57:28,106 - utils.wharton_processor - INFO - Successfully loaded local transcripts for 388904\n",
      "2025-04-29 21:57:28,107 - utils.wharton_processor - INFO - Successfully loaded 388904.csv\n",
      "2025-04-29 21:57:28,107 - utils.wharton_processor - INFO - 312932093\n",
      "2025-04-29 21:57:28,110 - utils.wharton_processor - INFO - Successfully loaded local transcripts for 312932093\n",
      "2025-04-29 21:57:28,110 - utils.wharton_processor - INFO - Successfully loaded 312932093.csv\n",
      "2025-04-29 21:57:28,113 - utils.wharton_processor - INFO - Successfully combined all matching transcripts: (165, 9)\n"
     ]
    }
   ],
   "source": [
    "transcripts = load_all_available_transcripts()\n",
    "transcripts = transcripts[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts = transcripts.full_text.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 22:00:11,037 - SummarizationPipeline - INFO - Initializing pipeline with model facebook/bart-large-cnn\n",
      "2025-04-29 22:00:11,041 - SummarizationPipeline - INFO - Loading model and tokenizer from local path: ../models/ragsum-facebook/bart-large-cnn-billsum\n",
      "/workspace/persistent/ragsum/venv/lib/python3.10/site-packages/transformers/models/bart/configuration_bart.py:176: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n",
      "  warnings.warn(\n",
      "2025-04-29 22:00:12,877 - SummarizationPipeline - INFO - Local load complete. Model is standard type, 1024 max length.\n",
      "2025-04-29 22:00:12,881 - utils.text_chunker - INFO - Initialized TextChunker with chunk_size=1024, chunk_overlap=102, prefix=\"\"\n",
      "2025-04-29 22:00:12,890 - utils.text_chunker - INFO - Starting text chunking...\n",
      "Chunking text: 100%|██████████| 13/13 [00:00<00:00, 142365.41it/s]\n",
      "2025-04-29 22:00:12,899 - utils.text_chunker - INFO - Text successfully split into 13 chunks.\n",
      "2025-04-29 22:00:12,899 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:00:13,575 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:00:14,515 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:00:15,417 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:00:16,265 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:00:17,208 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:00:18,153 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:00:19,156 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:00:20,096 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:00:20,917 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:00:21,756 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:00:22,420 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:00:23,360 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:00:24,241 - utils.text_chunker - INFO - Starting text chunking...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking text: 100%|██████████| 2/2 [00:00<00:00, 29537.35it/s]\n",
      "2025-04-29 22:00:24,245 - utils.text_chunker - INFO - Text successfully split into 2 chunks.\n",
      "2025-04-29 22:00:24,246 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:00:25,190 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 22:00:26,441 - SummarizationPipeline - INFO - Initializing pipeline with model google-t5/t5-base\n",
      "2025-04-29 22:00:26,443 - SummarizationPipeline - INFO - Loading model and tokenizer from local path: ../models/ragsum-google-t5/t5-base-billsum\n",
      "2025-04-29 22:00:27,435 - SummarizationPipeline - INFO - Local load complete. Model is standard type, 1024 max length.\n",
      "2025-04-29 22:00:27,440 - utils.text_chunker - INFO - Initialized TextChunker with chunk_size=1024, chunk_overlap=102, prefix=\"\"\n",
      "2025-04-29 22:00:27,447 - utils.text_chunker - INFO - Starting text chunking...\n",
      "Chunking text: 100%|██████████| 13/13 [00:00<00:00, 132024.10it/s]\n",
      "2025-04-29 22:00:27,454 - utils.text_chunker - INFO - Text successfully split into 13 chunks.\n",
      "2025-04-29 22:00:27,455 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:00:28,255 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:00:29,060 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:00:29,997 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:00:30,909 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:00:31,607 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:00:32,348 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:00:32,971 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:00:33,693 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:00:34,315 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:00:34,990 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:00:35,824 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:00:36,609 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 22:00:37,559 - SummarizationPipeline - INFO - Initializing pipeline with model google/pegasus-x-large\n",
      "2025-04-29 22:00:37,561 - SummarizationPipeline - INFO - Loading model and tokenizer from local path: ../models/ragsum-google/pegasus-x-large-billsum\n",
      "2025-04-29 22:00:40,481 - SummarizationPipeline - INFO - Local load complete. Model is pegasus type, 1024 max length.\n",
      "2025-04-29 22:00:40,482 - utils.text_chunker - INFO - Initialized TextChunker with chunk_size=1024, chunk_overlap=102, prefix=\"\"\n",
      "2025-04-29 22:00:40,504 - utils.text_chunker - INFO - Starting text chunking...\n",
      "Chunking text: 100%|██████████| 13/13 [00:00<00:00, 134965.23it/s]\n",
      "2025-04-29 22:00:40,513 - utils.text_chunker - INFO - Text successfully split into 13 chunks.\n",
      "2025-04-29 22:00:40,513 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:00:42,520 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:00:44,125 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:00:45,454 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:00:47,403 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:00:49,072 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:00:50,356 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:00:52,230 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:00:53,989 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:00:55,248 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:00:56,871 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:00:58,835 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:01:00,208 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1066 > 1024). Running this sequence through the model will result in indexing errors\n",
      "2025-04-29 22:01:01,717 - utils.text_chunker - INFO - Starting text chunking...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking text: 100%|██████████| 2/2 [00:00<00:00, 37786.52it/s]\n",
      "2025-04-29 22:01:01,720 - utils.text_chunker - INFO - Text successfully split into 2 chunks.\n",
      "2025-04-29 22:01:01,720 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:01:03,909 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 22:01:05,889 - SummarizationPipeline - INFO - Initializing pipeline with model human-centered-summarization/financial-summarization-pegasus\n",
      "2025-04-29 22:01:05,891 - SummarizationPipeline - INFO - Loading model and tokenizer from local path: ../models/ragsum-human-centered-summarization/financial-summarization-pegasus-billsum\n",
      "2025-04-29 22:01:07,884 - SummarizationPipeline - INFO - Local load complete. Model is pegasus type, 512 max length.\n",
      "2025-04-29 22:01:07,885 - utils.text_chunker - INFO - Initialized TextChunker with chunk_size=512, chunk_overlap=51, prefix=\"\"\n",
      "2025-04-29 22:01:07,902 - utils.text_chunker - INFO - Starting text chunking...\n",
      "Chunking text: 100%|██████████| 26/26 [00:00<00:00, 385342.42it/s]\n",
      "2025-04-29 22:01:07,909 - utils.text_chunker - INFO - Text successfully split into 26 chunks.\n",
      "2025-04-29 22:01:07,909 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:01:08,781 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:01:09,698 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:01:10,599 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:01:11,366 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:01:12,257 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:01:13,091 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:01:13,896 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:01:14,901 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:01:15,823 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:01:16,651 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:01:17,509 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:01:18,283 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:01:19,230 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:01:20,019 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:01:20,847 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:01:21,737 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:01:22,489 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:01:23,308 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:01:24,099 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:01:24,847 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:01:25,623 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:01:26,372 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:01:27,134 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:01:27,948 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:01:28,837 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1416 > 512). Running this sequence through the model will result in indexing errors\n",
      "2025-04-29 22:01:29,744 - utils.text_chunker - INFO - Starting text chunking...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking text: 100%|██████████| 4/4 [00:00<00:00, 71089.90it/s]\n",
      "2025-04-29 22:01:29,748 - utils.text_chunker - INFO - Text successfully split into 4 chunks.\n",
      "2025-04-29 22:01:29,748 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:01:30,572 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:01:31,419 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-29 22:01:32,152 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 2\n"
     ]
    }
   ],
   "source": [
    "checkpoints = ['facebook/bart-large-cnn', 'google-t5/t5-base', 'google/pegasus-x-large', 'human-centered-summarization/financial-summarization-pegasus']\n",
    "\n",
    "local_paths = [f'../models/ragsum-{checkpoint}-billsum' for checkpoint in checkpoints]\n",
    "\n",
    "logging_config: LoggingConfig = LoggingConfig()\n",
    "\n",
    "summaries = []\n",
    "\n",
    "for checkpoint, path in zip(checkpoints, local_paths):\n",
    "\n",
    "    model_config: ModelConfig = ModelConfig(\n",
    "        model_name_or_path=checkpoint, device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    )\n",
    "\n",
    "    pipeline = SummarizationPipeline(model_config=model_config, logging_config=logging_config, remote=False)\n",
    "    pipeline.load_from_local(path)\n",
    "    \n",
    "    tokenizer = pipeline.get_tokenizer()\n",
    "    chunker = TextChunker(tokenizer)\n",
    "\n",
    "    chunks = chunker.chunk_text(transcripts[0])\n",
    "\n",
    "    try:\n",
    "        chunk_summaries = [pipeline.summarize(chunk) for chunk in chunks]\n",
    "\n",
    "\n",
    "        combined_summary = \" \".join(chunk_summaries)\n",
    "\n",
    "        max_rounds = 5\n",
    "        round_count = 0\n",
    "        final_summary = \"\"\n",
    "        while round_count < max_rounds:\n",
    "            print(f'round {round_count+1}')\n",
    "\n",
    "            input_ids = tokenizer(combined_summary, return_tensors='pt', truncation=False)['input_ids']\n",
    "            if input_ids.shape[1] <= min(1024, pipeline.model_max_length):\n",
    "                final_summary = combined_summary\n",
    "                break\n",
    "            \n",
    "            re_chunks = chunker.chunk_text(combined_summary)\n",
    "            re_chunk_summaries = [pipeline.summarize(chunk) for chunk in re_chunks]\n",
    "            combined_summary = \" \".join(re_chunk_summaries)\n",
    "            round_count += 1\n",
    "        else:\n",
    "            final_summary = combined_summary\n",
    "\n",
    "        summaries.append(final_summary)\n",
    "    finally:\n",
    "        del pipeline\n",
    "        del model_config\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"JPMorgan's 21st Annual Technology and Automotive Investor Forum is held at the Consumer Electronics Show (CES) in Las Vegas, Nevada. JPMorgan is a sponsor of the conference, and the conference is open to the public. JPMorgan's semiconductor and semiconductor capital equipment analyst Harlan Sur:  Oren architecture has been adopted by Mercedes in their next-generation factories similar to some of the other automotive companies that are really seeing the need of the digital twin of the Ampere:  have seen is probably refreshes of the most important gamers out there to be a little bit over three years.  I think we'll see both new gamers and that refresh.  Siemens:  We've already probably passed an inflection point that we're seeing the growth of Orin with our NEVs. The next phase will be coming with our design wins both on robotaxis and bringing some of the early AV to market.\",\n",
       " \"asian-americans are a big part of the tech industry, and we're excited about the future. we're also excited about the future of our data center and our gaming platform. we're also excited about the future of our gaming and automotive business. if we hit your guidance for calendar '22, revenue for calendar '22 is expected to remain flattish. we're bringing in our latest DPU to market with BlueField. we're also bringing in our gaming business with our 40 Series with the kickoff of the 4090. the team is working on a number of things, including the end of the holiday season and the focus on generative AI. the team is also working on the end of Moore's law and what accelerated computing needs to do. the team is also working on the next-generation Hopper ramp, the H-100 Ram, continued strong enterprise adoption and strong network demand pool. Hopper is a full lineup across the board. Grace CPU is focused on very large models focusing on AI, focusing on the supercomputing, focusing on digital twins and the amount of work that we see in the future. Grace is expected to sample in the first half of the year, ramp into the market in the second half of the year. ADA 40 series is a new generation of RTX GPUs. the company is bringing a new generation of the product to market 2-plus years. ADA is also focusing on a new kind of supercomputing, such as digital twins. team has an $11 billion design win pipeline that is expected to unfold over the next few years. we're seeing the growth of Oren with our NEVs, which are starting to ramp. we're also seeing the revenue-sharing opportunities with Daimler and Jaguar Land Rover. NVIDIA team is driving many of the trends that you're going to hear about today. team is also driving an emerging software and services revenue stream. team is also focusing on bringing to market a large lineup of new products. CES is a great opportunity for us to talk about our gaming and data center business. we're also bringing our 40 Series to notebooks and our DLSS to the cloud. we're also excited about our automotive partnership with a big-name NEV maker. DLSS is focusing on the environment of data center computing as a whole. the team is working on a number of different business areas. we're also working on our automotive business, which is growing at a steady pace. the company is forecasting cloud CapEx spending still up high single digits this year. but more of the spending is focused on strategic compute initiatives like accelerated compute. the company is also focusing on the end of Moore's law and generative AI. despite a challenging consumer market, demand for the new Ada 40 Series gaming products seems quite good. 4090 sold out quite quickly, and 4080 GPU card pricing is marked up about 10%, 15% relative to MSRP. despite the challenges, the new product is a great opportunity for the company to expand into new markets. team has an $11 billion design win pipeline that is expected to unfold over the next few years. we're seeing 100% year-over-year growth in automotive, driven by Orin-based platforms. we're also seeing revenue-sharing opportunities with Daimler, Jaguar Land Rover in front of us. a lot of our work is focused on developing and bringing electric cars to market. we're still in the hardware infrastructure that's driving our revenue. but we're also essentially one architecture across everything that we do.\",\n",
       " \"And so you combine this with the next-generation Hopper ramp, your H-100 Ram, continued strong enterprise adoption, strong networking demand pool, history would suggest that the team should grow the data center segment by double-digit percentage growth rate this year. as the future of 3D Internet and folks really working not only on those 3 dimensionals but what you may see in terms of digital twins and work in that piece. For many years, we have put software in both the ecosystem as well as in our products that enables customers to take out of a box and begin their work that they need because we've enabled a lot of that software.\",\n",
       " \"Executive Vice President and Chief Financial Officer, Colette Kress, will speak at JPMorgan's 21st Annual Technology and Automotive Investor Forum. Executives: We are really pleased with what we're seeing with our ADA architecture, and we are still working on inventory correction CEO: We're on track for sampling in the first half, and we'll bring this to market in the second half of the year. Analysts expect Nvidia to report fourth-quarter revenues of $5.3 billion Here is the full webcast of Nvidia's special address at CES: Nvidia says 40% of its installed base has adopted GPUs. CEO: We're in the right position to both improve performance of nearly 5x or more from last generation Analysts: What are some of the challenges you're facing in the near term? Analysts: What are some of the challenges you'll face in the next 12 months? Executives: Well, we've got a lot of new products to bring to market, and we're always looking for new areas to find efficiencies.\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
