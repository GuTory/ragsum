{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarization with fine-tuned summarization models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/persistent/ragsum/venv/lib/python3.10/site-packages/_distutils_hack/__init__.py:53: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from utils import SummarizationPipeline, ModelConfig, LoggingConfig, load_all_available_transcripts, TextChunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 11:45:32,879 - utils.loaders - INFO - 32307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 11:45:33,599 - utils.loaders - INFO - Successfully loaded local transcripts for 32307\n",
      "2025-04-30 11:45:33,600 - utils.loaders - INFO - Successfully loaded 32307.csv\n",
      "2025-04-30 11:45:33,601 - utils.loaders - INFO - 126475\n",
      "2025-04-30 11:45:33,711 - utils.loaders - INFO - Successfully loaded local transcripts for 126475\n",
      "2025-04-30 11:45:33,712 - utils.loaders - INFO - Successfully loaded 126475.csv\n",
      "2025-04-30 11:45:33,712 - utils.loaders - INFO - 26446\n",
      "2025-04-30 11:45:33,759 - utils.loaders - INFO - Successfully loaded local transcripts for 26446\n",
      "2025-04-30 11:45:33,760 - utils.loaders - INFO - Successfully loaded 26446.csv\n",
      "2025-04-30 11:45:33,761 - utils.loaders - INFO - 388904\n",
      "2025-04-30 11:45:33,847 - utils.loaders - INFO - Successfully loaded local transcripts for 388904\n",
      "2025-04-30 11:45:33,847 - utils.loaders - INFO - Successfully loaded 388904.csv\n",
      "2025-04-30 11:45:33,848 - utils.loaders - INFO - 312932093\n",
      "2025-04-30 11:45:33,885 - utils.loaders - INFO - Successfully loaded local transcripts for 312932093\n",
      "2025-04-30 11:45:33,886 - utils.loaders - INFO - Successfully loaded 312932093.csv\n",
      "2025-04-30 11:45:33,887 - utils.loaders - INFO - Successfully combined all matching transcripts: (165, 9)\n"
     ]
    }
   ],
   "source": [
    "transcripts = load_all_available_transcripts()\n",
    "transcripts = transcripts[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts = transcripts.full_text.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 11:45:38,113 - SummarizationPipeline - INFO - Initializing pipeline with model facebook/bart-large-cnn\n",
      "2025-04-30 11:45:38,115 - SummarizationPipeline - INFO - Loading model and tokenizer from local path: ../models/ragsum-facebook/bart-large-cnn-billsum\n",
      "/workspace/persistent/ragsum/venv/lib/python3.10/site-packages/transformers/models/bart/configuration_bart.py:176: UserWarning: Please make sure the config includes `forced_bos_token_id=0` in future versions. The config can simply be saved and uploaded again to be fixed.\n",
      "  warnings.warn(\n",
      "2025-04-30 11:46:03,279 - SummarizationPipeline - INFO - Local load complete. Model is standard type, 1024 max length.\n",
      "2025-04-30 11:46:03,654 - utils.text_chunker - INFO - Initialized TextChunker with chunk_size=1024, chunk_overlap=102, prefix=\"\"\n",
      "2025-04-30 11:46:03,655 - utils.text_chunker - INFO - Starting text chunking...\n",
      "Chunking text: 100%|██████████| 13/13 [00:00<00:00, 197557.80it/s]\n",
      "2025-04-30 11:46:03,662 - utils.text_chunker - INFO - Text successfully split into 13 chunks.\n",
      "2025-04-30 11:46:03,663 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:09,171 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:10,126 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:11,037 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:11,917 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:12,875 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:13,847 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:14,859 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:15,835 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:16,667 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:17,522 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:18,192 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:19,153 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:20,046 - utils.text_chunker - INFO - Starting text chunking...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking text: 100%|██████████| 2/2 [00:00<00:00, 35394.97it/s]\n",
      "2025-04-30 11:46:20,050 - utils.text_chunker - INFO - Text successfully split into 2 chunks.\n",
      "2025-04-30 11:46:20,051 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:21,005 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:21,912 - utils.text_chunker - INFO - Starting text chunking...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking text: 100%|██████████| 16/16 [00:00<00:00, 262144.00it/s]\n",
      "2025-04-30 11:46:21,921 - utils.text_chunker - INFO - Text successfully split into 16 chunks.\n",
      "2025-04-30 11:46:21,921 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:22,876 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:23,830 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:24,785 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:25,691 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:26,289 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:27,105 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:27,906 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:28,879 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:29,763 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:30,720 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:31,692 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:32,654 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:33,610 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:34,309 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:35,263 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:36,236 - utils.text_chunker - INFO - Starting text chunking...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking text: 100%|██████████| 2/2 [00:00<00:00, 33288.13it/s]\n",
      "2025-04-30 11:46:36,240 - utils.text_chunker - INFO - Text successfully split into 2 chunks.\n",
      "2025-04-30 11:46:36,241 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:37,170 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:37,873 - utils.text_chunker - INFO - Starting text chunking...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking text: 100%|██████████| 24/24 [00:00<00:00, 394758.02it/s]\n",
      "2025-04-30 11:46:37,883 - utils.text_chunker - INFO - Text successfully split into 24 chunks.\n",
      "2025-04-30 11:46:37,883 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:38,528 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:39,443 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:40,396 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:41,179 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:42,132 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:43,083 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:44,036 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:44,991 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:45,941 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:46,895 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:47,851 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:48,806 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:49,756 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:50,711 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:51,660 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:52,501 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:53,456 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:54,377 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:55,188 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:56,144 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:56,871 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:57,825 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:58,781 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:46:59,752 - utils.text_chunker - INFO - Starting text chunking...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking text: 100%|██████████| 3/3 [00:00<00:00, 44306.03it/s]\n",
      "2025-04-30 11:46:59,756 - utils.text_chunker - INFO - Text successfully split into 3 chunks.\n",
      "2025-04-30 11:46:59,757 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:00,389 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:01,343 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 11:47:02,625 - SummarizationPipeline - INFO - Initializing pipeline with model google-t5/t5-base\n",
      "2025-04-30 11:47:02,626 - SummarizationPipeline - INFO - Loading model and tokenizer from local path: ../models/ragsum-google-t5/t5-base-billsum\n",
      "2025-04-30 11:47:15,721 - SummarizationPipeline - INFO - Local load complete. Model is standard type, 1024 max length.\n",
      "2025-04-30 11:47:15,725 - utils.text_chunker - INFO - Initialized TextChunker with chunk_size=1024, chunk_overlap=102, prefix=\"\"\n",
      "2025-04-30 11:47:15,734 - utils.text_chunker - INFO - Starting text chunking...\n",
      "Chunking text: 100%|██████████| 13/13 [00:00<00:00, 138742.88it/s]\n",
      "2025-04-30 11:47:15,741 - utils.text_chunker - INFO - Text successfully split into 13 chunks.\n",
      "2025-04-30 11:47:15,741 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:17,353 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:18,173 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:19,140 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:20,067 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:20,777 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:21,527 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:22,160 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:22,906 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:23,538 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:24,217 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:25,065 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:25,856 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:26,464 - utils.text_chunker - INFO - Starting text chunking...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking text: 100%|██████████| 16/16 [00:00<00:00, 261123.98it/s]\n",
      "2025-04-30 11:47:26,473 - utils.text_chunker - INFO - Text successfully split into 16 chunks.\n",
      "2025-04-30 11:47:26,474 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:27,451 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:28,348 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:29,177 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:30,073 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:30,726 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:31,494 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:32,052 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:32,937 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:33,639 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:34,420 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:35,319 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:36,205 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:36,861 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:37,632 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:38,167 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:39,053 - utils.text_chunker - INFO - Starting text chunking...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking text: 100%|██████████| 24/24 [00:00<00:00, 330043.59it/s]\n",
      "2025-04-30 11:47:39,063 - utils.text_chunker - INFO - Text successfully split into 24 chunks.\n",
      "2025-04-30 11:47:39,064 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:39,990 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:40,514 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:41,307 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:41,852 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:42,905 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:43,592 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:44,389 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:45,304 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:46,340 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:47,383 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:48,319 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:49,049 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:49,947 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:50,526 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:51,370 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:51,889 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:52,833 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:53,434 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:54,007 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:54,747 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:55,539 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:56,559 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:57,303 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:58,000 - utils.text_chunker - INFO - Starting text chunking...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking text: 100%|██████████| 2/2 [00:00<00:00, 35544.95it/s]\n",
      "2025-04-30 11:47:58,003 - utils.text_chunker - INFO - Text successfully split into 2 chunks.\n",
      "2025-04-30 11:47:58,004 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:47:58,637 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 11:47:59,728 - SummarizationPipeline - INFO - Initializing pipeline with model google/pegasus-x-large\n",
      "2025-04-30 11:47:59,729 - SummarizationPipeline - INFO - Loading model and tokenizer from local path: ../models/ragsum-google/pegasus-x-large-billsum\n",
      "2025-04-30 11:48:32,339 - SummarizationPipeline - INFO - Local load complete. Model is pegasus type, 1024 max length.\n",
      "2025-04-30 11:48:32,340 - utils.text_chunker - INFO - Initialized TextChunker with chunk_size=1024, chunk_overlap=102, prefix=\"\"\n",
      "2025-04-30 11:48:32,358 - utils.text_chunker - INFO - Starting text chunking...\n",
      "Chunking text: 100%|██████████| 13/13 [00:00<00:00, 213827.26it/s]\n",
      "2025-04-30 11:48:32,366 - utils.text_chunker - INFO - Text successfully split into 13 chunks.\n",
      "2025-04-30 11:48:32,367 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:48:34,423 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:48:36,062 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:48:37,439 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:48:39,433 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:48:41,143 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:48:42,454 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:48:44,372 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:48:46,178 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:48:47,477 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:48:49,148 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:48:51,165 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:48:52,589 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1066 > 1024). Running this sequence through the model will result in indexing errors\n",
      "2025-04-30 11:48:54,142 - utils.text_chunker - INFO - Starting text chunking...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking text: 100%|██████████| 2/2 [00:00<00:00, 33689.19it/s]\n",
      "2025-04-30 11:48:54,145 - utils.text_chunker - INFO - Text successfully split into 2 chunks.\n",
      "2025-04-30 11:48:54,145 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:48:56,395 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:48:58,066 - utils.text_chunker - INFO - Starting text chunking...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking text: 100%|██████████| 16/16 [00:00<00:00, 226719.14it/s]\n",
      "2025-04-30 11:48:58,074 - utils.text_chunker - INFO - Text successfully split into 16 chunks.\n",
      "2025-04-30 11:48:58,074 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:48:59,773 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:49:02,037 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:49:03,760 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:49:05,348 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:49:07,223 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:49:08,777 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:49:10,502 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:49:12,139 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:49:14,152 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:49:16,606 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:49:18,206 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:49:19,837 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:49:21,838 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:49:23,287 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:49:24,336 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:49:25,990 - utils.text_chunker - INFO - Starting text chunking...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking text: 100%|██████████| 2/2 [00:00<00:00, 37117.73it/s]\n",
      "2025-04-30 11:49:25,994 - utils.text_chunker - INFO - Text successfully split into 2 chunks.\n",
      "2025-04-30 11:49:25,994 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:49:27,695 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:49:29,044 - utils.text_chunker - INFO - Starting text chunking...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking text: 100%|██████████| 24/24 [00:00<00:00, 282762.07it/s]\n",
      "2025-04-30 11:49:29,054 - utils.text_chunker - INFO - Text successfully split into 24 chunks.\n",
      "2025-04-30 11:49:29,055 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:49:30,566 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:49:32,451 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:49:33,795 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:49:35,132 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:49:37,299 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:49:39,566 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:49:42,188 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:49:45,194 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:49:49,064 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:49:50,515 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:49:52,214 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:49:55,184 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:49:56,764 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:49:58,637 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:49:59,918 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:50:01,494 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:50:03,120 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:50:06,645 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:50:08,341 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:50:10,274 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:50:12,650 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:50:13,884 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:50:15,530 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:50:17,236 - utils.text_chunker - INFO - Starting text chunking...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking text: 100%|██████████| 3/3 [00:00<00:00, 53544.31it/s]\n",
      "2025-04-30 11:50:17,241 - utils.text_chunker - INFO - Text successfully split into 3 chunks.\n",
      "2025-04-30 11:50:17,241 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:50:19,007 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:50:21,270 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 11:50:24,185 - SummarizationPipeline - INFO - Initializing pipeline with model human-centered-summarization/financial-summarization-pegasus\n",
      "2025-04-30 11:50:24,186 - SummarizationPipeline - INFO - Loading model and tokenizer from local path: ../models/ragsum-human-centered-summarization/financial-summarization-pegasus-billsum\n",
      "2025-04-30 11:50:52,389 - SummarizationPipeline - INFO - Local load complete. Model is pegasus type, 512 max length.\n",
      "2025-04-30 11:50:52,390 - utils.text_chunker - INFO - Initialized TextChunker with chunk_size=512, chunk_overlap=51, prefix=\"\"\n",
      "2025-04-30 11:50:52,405 - utils.text_chunker - INFO - Starting text chunking...\n",
      "Chunking text: 100%|██████████| 26/26 [00:00<00:00, 392273.04it/s]\n",
      "2025-04-30 11:50:52,413 - utils.text_chunker - INFO - Text successfully split into 26 chunks.\n",
      "2025-04-30 11:50:52,413 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:50:53,314 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:50:54,254 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:50:55,164 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:50:55,939 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:50:56,848 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:50:57,702 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:50:58,518 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:50:59,533 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:00,454 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:01,275 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:02,142 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:02,920 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:03,882 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:04,687 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:05,520 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:06,424 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:07,178 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:08,021 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:08,827 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:09,590 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:10,378 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:11,142 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:11,919 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:12,736 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:13,639 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1416 > 512). Running this sequence through the model will result in indexing errors\n",
      "2025-04-30 11:51:14,567 - utils.text_chunker - INFO - Starting text chunking...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking text: 100%|██████████| 4/4 [00:00<00:00, 66841.50it/s]\n",
      "2025-04-30 11:51:14,571 - utils.text_chunker - INFO - Text successfully split into 4 chunks.\n",
      "2025-04-30 11:51:14,571 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:15,410 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:16,278 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:17,029 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:17,780 - utils.text_chunker - INFO - Starting text chunking...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking text: 100%|██████████| 33/33 [00:00<00:00, 453809.94it/s]\n",
      "2025-04-30 11:51:17,788 - utils.text_chunker - INFO - Text successfully split into 33 chunks.\n",
      "2025-04-30 11:51:17,789 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:18,640 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:19,646 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:20,381 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:21,244 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:22,007 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:22,818 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:23,770 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:24,538 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:25,318 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:26,168 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:26,973 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:27,729 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:28,613 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:29,360 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:30,112 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:30,931 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:31,830 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:32,625 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:33,387 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:34,265 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:35,008 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:35,758 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:36,586 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:37,427 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:38,212 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:39,036 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:39,814 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:40,645 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:41,386 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:42,130 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:42,893 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:43,801 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:44,635 - utils.text_chunker - INFO - Starting text chunking...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking text: 100%|██████████| 4/4 [00:00<00:00, 71089.90it/s]\n",
      "2025-04-30 11:51:44,638 - utils.text_chunker - INFO - Text successfully split into 4 chunks.\n",
      "2025-04-30 11:51:44,638 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:45,493 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:46,277 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:47,061 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:47,861 - utils.text_chunker - INFO - Starting text chunking...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking text: 100%|██████████| 47/47 [00:00<00:00, 531353.88it/s]\n",
      "2025-04-30 11:51:47,871 - utils.text_chunker - INFO - Text successfully split into 47 chunks.\n",
      "2025-04-30 11:51:47,872 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:48,973 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:49,739 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:50,610 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:51,345 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:52,194 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:52,991 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:53,833 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:54,585 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:55,339 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:56,163 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:56,915 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:57,906 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:58,719 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:51:59,472 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:52:00,225 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:52:00,976 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:52:02,019 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:52:02,804 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:52:03,632 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:52:04,379 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:52:05,145 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:52:05,966 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:52:06,862 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:52:07,660 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:52:08,530 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:52:09,397 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:52:10,322 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:52:11,074 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:52:12,033 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:52:12,790 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:52:13,686 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:52:14,468 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:52:15,295 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:52:16,098 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:52:16,906 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:52:17,688 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:52:18,508 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:52:19,280 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:52:20,097 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:52:20,858 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:52:21,605 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:52:22,379 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:52:23,234 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:52:23,987 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:52:24,845 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:52:25,653 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:52:26,429 - utils.text_chunker - INFO - Starting text chunking...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking text: 100%|██████████| 6/6 [00:00<00:00, 102717.65it/s]\n",
      "2025-04-30 11:52:26,433 - utils.text_chunker - INFO - Text successfully split into 6 chunks.\n",
      "2025-04-30 11:52:26,434 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:52:27,461 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:52:28,257 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:52:29,214 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:52:29,988 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:52:30,737 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 2\n"
     ]
    }
   ],
   "source": [
    "checkpoints = ['facebook/bart-large-cnn', 'google-t5/t5-base', 'google/pegasus-x-large', 'human-centered-summarization/financial-summarization-pegasus']\n",
    "\n",
    "local_paths = [f'../models/ragsum-{checkpoint}-billsum' for checkpoint in checkpoints]\n",
    "\n",
    "logging_config: LoggingConfig = LoggingConfig()\n",
    "\n",
    "summaries = []\n",
    "\n",
    "for checkpoint, path in zip(checkpoints, local_paths):\n",
    "\n",
    "    model_config: ModelConfig = ModelConfig(\n",
    "        model_name_or_path=checkpoint, device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    )\n",
    "\n",
    "    pipeline = SummarizationPipeline(model_config=model_config, logging_config=logging_config, remote=False)\n",
    "    pipeline.load_from_local(path)\n",
    "    \n",
    "    tokenizer = pipeline.get_tokenizer()\n",
    "    chunker = TextChunker(tokenizer)\n",
    "\n",
    "    try:\n",
    "        for transcript in tqdm(transcripts):\n",
    "            chunks = chunker.chunk_text(transcript)\n",
    "            chunk_summaries = [pipeline.summarize(chunk) for chunk in chunks]\n",
    "            combined_summary = \" \".join(chunk_summaries)\n",
    "            max_rounds = 5\n",
    "            round_count = 0\n",
    "            final_summary = \"\"\n",
    "            while round_count < max_rounds:\n",
    "                print(f'round {round_count+1}')\n",
    "                input_ids = tokenizer(combined_summary, return_tensors='pt', truncation=False)['input_ids']\n",
    "                if input_ids.shape[1] <= min(1024, pipeline.model_max_length):\n",
    "                    final_summary = combined_summary\n",
    "                    break\n",
    "                re_chunks = chunker.chunk_text(combined_summary)\n",
    "                re_chunk_summaries = [pipeline.summarize(chunk) for chunk in re_chunks]\n",
    "                combined_summary = \" \".join(re_chunk_summaries)\n",
    "                round_count += 1\n",
    "            else:\n",
    "                final_summary = combined_summary\n",
    "            summaries.append(final_summary)\n",
    "    finally:\n",
    "        del pipeline\n",
    "        del model_config\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"JPMorgan's 21st Annual Technology and Automotive Investor Forum is held at the Consumer Electronics Show (CES) in Las Vegas, Nevada. JPMorgan is a sponsor of the conference, and the conference is open to the public. JPMorgan's semiconductor and semiconductor capital equipment analyst Harlan Sur:  Oren architecture has been adopted by Mercedes in their next-generation factories similar to some of the other automotive companies that are really seeing the need of the digital twin of the Ampere:  have seen is probably refreshes of the most important gamers out there to be a little bit over three years.  I think we'll see both new gamers and that refresh.  Siemens:  We've already probably passed an inflection point that we're seeing the growth of Orin with our NEVs. The next phase will be coming with our design wins both on robotaxis and bringing some of the early AV to market.\",\n",
       " \"JPMorgan's 41st Annual Healthcare Conference:  Presenter:  Kimberly Powell, Vice President of Healthcare at NVIDIA (NASDAQ: NVDA) Describes NVIDIA as a leader in accelerated computing semiconductor systems, hardware and software platforms, in areas like artificial intelligence and deep learning, powering some of the world's most powerful supercomputers and driving compute innovation for cloud and hyperscalers as well as large vertical markets like health care and life sciences. Data center business for NVIDIA has grown at a 70% CAGR over the past three years, 60% of its total revenues, and accelerated compute spending within health care continues to grow at an extremely rapid rate. The company believes that we could be the next $1 billion industry for the data center value proposition that NVIDIA can underpin.\",\n",
       " \"Learn about how to boost productivity and accelerate real-time design collaboration with NVIDIA's vGPU solutions.  Share your thoughts on the industry trends you're seeing in the AECO industry that have fundamentally driven the need for developing a robust GPU virtualization solution and share high-level info on NVIDIA's solutions. How to Boost Productivity and Accelerate Real-Time Design Collaboration: How to Boost Your Productivity with NVIDIA's A40 and A100 AI and Machine Learning Cards (A40s) and the A100s) This event is being presented by NVIDIA and HFA, the architectural and engineering design (AEC) company, to discuss the industry trends that have fundamentally driven the need for developing a robust GPU virtualization solution. John Raines and Brittany Pylant from HFA:  If you're ready to try out 2D power and 3D graphics virtualization, you can use it for free for 90 days, give it a try, just to our website n Nvidia.com to download.  You could also test drive the VMware on NVIDIA vGPU.  The company is using the 8q profiles because of our real power users, heavy in Enscape and SketchUp specifically and Revit, and so we've got some giant Revit Webs\",\n",
       " \"asian-americans are a big part of the tech industry, and we're excited about the future. we're also excited about the future of our data center and our gaming platform. we're also excited about the future of our gaming and automotive business. if we hit your guidance for calendar '22, revenue for calendar '22 is expected to remain flattish. we're bringing in our latest DPU to market with BlueField. we're also bringing in our gaming business with our 40 Series with the kickoff of the 4090. the team is working on a number of things, including the end of the holiday season and the focus on generative AI. the team is also working on the end of Moore's law and what accelerated computing needs to do. the team is also working on the next-generation Hopper ramp, the H-100 Ram, continued strong enterprise adoption and strong network demand pool. Hopper is a full lineup across the board. Grace CPU is focused on very large models focusing on AI, focusing on the supercomputing, focusing on digital twins and the amount of work that we see in the future. Grace is expected to sample in the first half of the year, ramp into the market in the second half of the year. ADA 40 series is a new generation of RTX GPUs. the company is bringing a new generation of the product to market 2-plus years. ADA is also focusing on a new kind of supercomputing, such as digital twins. team has an $11 billion design win pipeline that is expected to unfold over the next few years. we're seeing the growth of Oren with our NEVs, which are starting to ramp. we're also seeing the revenue-sharing opportunities with Daimler and Jaguar Land Rover. NVIDIA team is driving many of the trends that you're going to hear about today. team is also driving an emerging software and services revenue stream. team is also focusing on bringing to market a large lineup of new products. CES is a great opportunity for us to talk about our gaming and data center business. we're also bringing our 40 Series to notebooks and our DLSS to the cloud. we're also excited about our automotive partnership with a big-name NEV maker. DLSS is focusing on the environment of data center computing as a whole. the team is working on a number of different business areas. we're also working on our automotive business, which is growing at a steady pace. the company is forecasting cloud CapEx spending still up high single digits this year. but more of the spending is focused on strategic compute initiatives like accelerated compute. the company is also focusing on the end of Moore's law and generative AI. despite a challenging consumer market, demand for the new Ada 40 Series gaming products seems quite good. 4090 sold out quite quickly, and 4080 GPU card pricing is marked up about 10%, 15% relative to MSRP. despite the challenges, the new product is a great opportunity for the company to expand into new markets. team has an $11 billion design win pipeline that is expected to unfold over the next few years. we're seeing 100% year-over-year growth in automotive, driven by Orin-based platforms. we're also seeing revenue-sharing opportunities with Daimler, Jaguar Land Rover in front of us. a lot of our work is focused on developing and bringing electric cars to market. we're still in the hardware infrastructure that's driving our revenue. but we're also essentially one architecture across everything that we do.\",\n",
       " 'analyst: accelerated computing is the future of health care. he says we\\'re a time machine, but we\\'re also a full stack company. he says we\\'re a leader in accelerating computing, deep learning and visualization. he says we\\'re a leader in the health care industry, and we\\'re excited to share it with you. nvidia is building an end-to-end computing platform to serve the medical device market. nvidia is partnering across the genomics industry from new sequencers to cloud services and large pharma. nvidia is accelerating 96 optical genome mapping, a new ion-based x-ray scanner. generative AI is seeing broad applicability across life sciences and drug discovery. a team of scientists at meta, roast lab, barsele lab and a team of scientists at meta are working on a new language model. the team is working on a new approach to predicting the evolution of a virus. a collaboration between NVIDIA and instadeep has resulted in a state-of-the-art genomics language model. the model is able to generalize across many tasks, and is able to generate a library of those. the model is a generative AI model for protein engineering called pro TVAE. NVIDIA is building the compute platforms to address the breadth of health care. a huge explosion of seminal work happened in just the last 3 months of last year. a new generation of generative AI and accelerated computing is coming. a data center business for NVIDIA has grown at a 70% CAGR over the past 3 years. underneath that, accelerated compute spending within health care continues to grow at an extremely rapid rate. we believe that we could be the next $1 billion industry for NVIDIA. a huge amount of computing is happening in the pharmaceutical industry. a huge part of that is coming from simulation, and the balance is there. a huge part of that is coming from generative AI. the team just rolled out its next-generation compute acceleration platform, the H100. it\\'s based on the 4-nanometer, leading-edge manufacturing technology, 80 billion transistors on a single piece of silicon. throughout the first quarter of Q1, you\\'re going to see H100 popping up in all of the public clouds. asian-american semiconductor analyst Harlan sur joins us for the fourth time in 5 years. he says the company is a leader in accelerated computing semiconductor systems. sur: \"we\\'re rapidly becoming the de facto standard for surgical robotics platforms\" nvidia is building an end-to-end computing platform to serve the medical device market. nvidia is partnering across the genomics industry, from new sequencers to bioinformatics platforms. 2022 was an absolute breakout year for NVIDIA-accelerated genomics. a team of scientists has created a new language model for genomics. the model is able to learn context and meaning by tracking relationships in sequential data. the team is working on a new approach to predicting the evolution of a virus. the team is also working on a new approach to predicting the evolution of a virus. a collaboration between NVIDIA and InstaDeep has resulted in a state-of-the-art genomics language model. the model is able to generalize across many tasks, which otherwise were built model at a time quite narrowly. the team is also working with the leaders in biology and drug discovery field. NVIDIA is building the compute platforms to address the breadth of health care. a huge explosion of seminal work happened in just the last 3 months of last year. a new generation of generative AI and accelerated computing is coming. a full-stack company, NVIDIA, is growing at a 70% CAGR over the past 3 years. the company is focused on some very important large markets within the health care industry. the company is also focused on the pharmaceutical industry, which is building centers of excellence in AI. a huge chunk of the $1 billion in revenue is coming from generative AI. a huge chunk of that is coming from medical devices. a huge chunk of that is coming from simulations. the team just rolled out its next-generation compute acceleration platform, the H100. it\\'s based on the 4-nanometer, leading-edge manufacturing technology, 80 billion transistors on a single piece of silicon. throughout the first quarter of Q1, you\\'re going to see H100 popping up in all of the public clouds.',\n",
       " 'a lot of firms are looking to use a vGPU solution to help them achieve their goals. a lot of knowledge workers and creative professionals are using virtual desktops. a lot of people are using video conferencing tools. xenDesktops are a virtualized version of a desktop that can be used for visualizations. 0 to 257 people were virtualized in less than a year. vGPUs allow you to carve up and choose the different profiles to meet your needs.',\n",
       " \"And so you combine this with the next-generation Hopper ramp, your H-100 Ram, continued strong enterprise adoption, strong networking demand pool, history would suggest that the team should grow the data center segment by double-digit percentage growth rate this year. as the future of 3D Internet and folks really working not only on those 3 dimensionals but what you may see in terms of digital twins and work in that piece. For many years, we have put software in both the ecosystem as well as in our products that enables customers to take out of a box and begin their work that they need because we've enabled a lot of that software.\",\n",
       " \"For those of you that don't know NVIDIA, leader in accelerated computing semiconductor systems, hardware and software platforms, in areas like artificial intelligence and deep learning, powering some of the world's most powerful supercomputers and driving compute innovation for cloud and hyperscalers as well as large vertical markets like health care and life sciences. And then generative AI, like DALI, where you can go from one domain to another from text to an image, how could that not have massive utility in going through the different types of data sets in health care, from health records to genetic marker to what they saw in your image to what your pathology report is looking at?\",\n",
       " 'With that density though and reducing the amount of hardware required in your data center, it really helps reduce your footprint and sustainability is something I\\'m also pretty passionate about and making sure that we aren\\'t generating a lot of e-waste and virtualization is a great way to do that because what it allows us to do is instead of just we\\'re upgrading this set of users, time to take those old machines and throw them in the garbage or never look at them again in a [ supply ] closet until they get so big that they\\'re falling out of the closet. This has a lot of different applications.\" And so initially, we were thinking of using it as a way just for when we\\'re on site with a client to access for files and better show it versus having to load up a bunch of different files on a laptop and risk missing something where we could actually access our file servers in real time without having to set up a VPN connection. With that density though and reducing the amount of hardware required in your data center, it really helps reduce your footprint and sustainability is something I\\'m also pretty passionate about and making sure that we aren\\'t generating a lot of e-waste and virtualization is a great way to do that because what it allows us to do is instead of just we\\'re upgrading this set of users, time to take those old machines and throw them in the garbage or never look at them again in a supply closet until they get so big that they\\'re falling out of the closet.',\n",
       " \"Executive Vice President and Chief Financial Officer, Colette Kress, will speak at JPMorgan's 21st Annual Technology and Automotive Investor Forum. Executives: We are really pleased with what we're seeing with our ADA architecture, and we are still working on inventory correction CEO: We're on track for sampling in the first half, and we'll bring this to market in the second half of the year. Analysts expect Nvidia to report fourth-quarter revenues of $5.3 billion Here is the full webcast of Nvidia's special address at CES: Nvidia says 40% of its installed base has adopted GPUs. CEO: We're in the right position to both improve performance of nearly 5x or more from last generation Analysts: What are some of the challenges you're facing in the near term? Analysts: What are some of the challenges you'll face in the next 12 months? Executives: Well, we've got a lot of new products to bring to market, and we're always looking for new areas to find efficiencies.\",\n",
       " \"Company to present at JPMorgan's 41st Annual Healthcare Conference in San Francisco, California. Kimberly Powell, Vice President of Healthcare at Intel, will speak at the conference on Wednesday, September 5, at 10:20 am eastern time, and on Thursday, September 6, at 2:50 pm eastern time CEO: We are in a competitive landscape with every other AI company. Powell: It doesn't have to be that far of writing up a medical report and discharging a patient Analysts: What are some of the tailwinds for your health care franchise? CEO explains the company's approach to AI and its role in the health care industry. ProT-VAE is a machine-guided directed evolution model for protein engineering I want to go quickly next to a modality that is super, super exciting. Analysts ask Nvidia about split between big pharma and TechBio. CEO: We're coming into more of a 50-50 split, because of some of the dynamics that we're seeing in the market Analysts ask Intel about 3D printing adoption curve.\",\n",
       " \"Physically-based rendering, real-time design collaboration, and 3D printing are big trends in the industry. VDI is a good fit for AEC firms, says Autodesk VP of product management. HFA has been using the solution for years, and they say it's helping them to improve access, boost productivity, and accelerate design collaboration In our series of Q&A sessions, attendees share their experiences using Revit at work, at home, and on-the-go. In just under 12 hours, we went from 0 to 257 remote users, and that's something we were really proud of VDI is here to help solve some of the pain points for architects, designers, and others using the platform. AEC industry is seeing 4 major trends: real-time ray tracing, collaboration, faster decision-making, and more accurate renders. VDI has many different use cases that could leverage this technology. HFA has been using the technology for more than a decade to improve access, boost productivity and accelerate design collaboration Director of BIM at HFA explains how the firm works together as a team VDI World: Autodesk VP of product management explains how VDI has changed the way people work. HFA co-founder and CEO Andrew Green talks about the company's green initiatives in an interview with Techdirt's Chris O'Brien Executives: We have a few different products from that we're looking to test more. Attendees: No, we don't get to do a whole lot with AI Lenovo has been using the T4 motherboard in its data center for over a year\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
