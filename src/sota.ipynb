{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State-of-the-art hierarchical summarization with Huggingface models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/persistent/ragsum/venv/lib/python3.10/site-packages/_distutils_hack/__init__.py:53: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from utils import SummarizationPipeline, ModelConfig, LoggingConfig, load_all_available_transcripts, TextChunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 12:01:17,232 - utils.loaders - INFO - 32307\n",
      "2025-04-30 12:01:17,322 - utils.loaders - INFO - Successfully loaded local transcripts for 32307\n",
      "2025-04-30 12:01:17,323 - utils.loaders - INFO - Successfully loaded 32307.csv\n",
      "2025-04-30 12:01:17,324 - utils.loaders - INFO - 126475\n",
      "2025-04-30 12:01:17,346 - utils.loaders - INFO - Successfully loaded local transcripts for 126475\n",
      "2025-04-30 12:01:17,346 - utils.loaders - INFO - Successfully loaded 126475.csv\n",
      "2025-04-30 12:01:17,347 - utils.loaders - INFO - 26446\n",
      "2025-04-30 12:01:17,357 - utils.loaders - INFO - Successfully loaded local transcripts for 26446\n",
      "2025-04-30 12:01:17,357 - utils.loaders - INFO - Successfully loaded 26446.csv\n",
      "2025-04-30 12:01:17,357 - utils.loaders - INFO - 388904\n",
      "2025-04-30 12:01:17,375 - utils.loaders - INFO - Successfully loaded local transcripts for 388904\n",
      "2025-04-30 12:01:17,375 - utils.loaders - INFO - Successfully loaded 388904.csv\n",
      "2025-04-30 12:01:17,375 - utils.loaders - INFO - 312932093\n",
      "2025-04-30 12:01:17,378 - utils.loaders - INFO - Successfully loaded local transcripts for 312932093\n",
      "2025-04-30 12:01:17,378 - utils.loaders - INFO - Successfully loaded 312932093.csv\n",
      "2025-04-30 12:01:17,379 - utils.loaders - INFO - Successfully combined all matching transcripts: (165, 9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['companyid', 'companyname', 'mostimportantdateutc',\n",
       "       'mostimportanttimeutc', 'headline', 'full_text', 'uuid', 'word_count',\n",
       "       'word_count_nltk'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcripts = load_all_available_transcripts()\n",
    "transcripts.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 11:53:39,499 - utils.loaders - INFO - 32307\n",
      "2025-04-30 11:53:39,591 - utils.loaders - INFO - Successfully loaded local transcripts for 32307\n",
      "2025-04-30 11:53:39,592 - utils.loaders - INFO - Successfully loaded 32307.csv\n",
      "2025-04-30 11:53:39,592 - utils.loaders - INFO - 126475\n",
      "2025-04-30 11:53:39,615 - utils.loaders - INFO - Successfully loaded local transcripts for 126475\n",
      "2025-04-30 11:53:39,616 - utils.loaders - INFO - Successfully loaded 126475.csv\n",
      "2025-04-30 11:53:39,616 - utils.loaders - INFO - 26446\n",
      "2025-04-30 11:53:39,627 - utils.loaders - INFO - Successfully loaded local transcripts for 26446\n",
      "2025-04-30 11:53:39,628 - utils.loaders - INFO - Successfully loaded 26446.csv\n",
      "2025-04-30 11:53:39,629 - utils.loaders - INFO - 388904\n",
      "2025-04-30 11:53:39,647 - utils.loaders - INFO - Successfully loaded local transcripts for 388904\n",
      "2025-04-30 11:53:39,647 - utils.loaders - INFO - Successfully loaded 388904.csv\n",
      "2025-04-30 11:53:39,648 - utils.loaders - INFO - 312932093\n",
      "2025-04-30 11:53:39,650 - utils.loaders - INFO - Successfully loaded local transcripts for 312932093\n",
      "2025-04-30 11:53:39,651 - utils.loaders - INFO - Successfully loaded 312932093.csv\n",
      "2025-04-30 11:53:39,653 - utils.loaders - INFO - Successfully combined all matching transcripts: (165, 9)\n"
     ]
    }
   ],
   "source": [
    "transcripts = load_all_available_transcripts()\n",
    "transcripts = transcripts[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts = transcripts.full_text.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 11:53:39,683 - SummarizationPipeline - INFO - Initializing pipeline with model facebook/bart-large-cnn\n",
      "2025-04-30 11:53:39,685 - SummarizationPipeline - INFO - Loading tokenizer for facebook/bart-large-cnn\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a5321c7ef8b41b18608dcb70428c709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2692bdf6f88c41c985b526090c789dd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "943c52cdbfc14781a5ca541c4b49e0ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9554842d68cd452a96f25cc63627bd1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 11:53:42,295 - SummarizationPipeline - INFO - Loading model for facebook/bart-large-cnn (8bit=False, device=cuda)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dc8052e77a94e7bb87a4200201a3045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21a86a2673ef4c8e951c3c291c58bde9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 11:53:49,310 - SummarizationPipeline - INFO - Model and tokenizer loaded successfully. Model max length: 1000000000000000019884624838656\n",
      "2025-04-30 11:53:49,311 - SummarizationPipeline - INFO - Using prefix: summarize: \n",
      "2025-04-30 11:53:49,323 - SummarizationPipeline - WARNING - Model vocab size (50265) and tokenizer vocab size (50264) mismatch. Resizing model embeddings.\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "2025-04-30 11:53:51,294 - utils.text_chunker - INFO - Initialized TextChunker with chunk_size=1024, chunk_overlap=102, prefix=\"\"\n",
      "2025-04-30 11:53:51,296 - utils.text_chunker - INFO - Starting text chunking...\n",
      "Chunking text: 100%|██████████| 13/13 [00:00<00:00, 118534.68it/s]\n",
      "2025-04-30 11:53:51,304 - utils.text_chunker - INFO - Text successfully split into 13 chunks.\n",
      "2025-04-30 11:53:51,305 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:53:53,066 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:53:53,862 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:53:54,502 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:53:55,171 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:53:55,847 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:53:56,447 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:53:57,078 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:53:57,713 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:53:58,375 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:53:58,938 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:53:59,598 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:00,185 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:00,829 - utils.text_chunker - INFO - Starting text chunking...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking text: 100%|██████████| 16/16 [00:00<00:00, 295633.76it/s]\n",
      "2025-04-30 11:54:00,846 - utils.text_chunker - INFO - Text successfully split into 16 chunks.\n",
      "2025-04-30 11:54:00,846 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:01,558 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:02,144 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:02,764 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:03,546 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:04,196 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:04,806 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:05,525 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:06,107 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:06,817 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:07,482 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:08,152 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:08,843 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:09,544 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:10,136 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:10,976 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:11,557 - utils.text_chunker - INFO - Starting text chunking...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking text: 100%|██████████| 2/2 [00:00<00:00, 41120.63it/s]\n",
      "2025-04-30 11:54:11,561 - utils.text_chunker - INFO - Text successfully split into 2 chunks.\n",
      "2025-04-30 11:54:11,561 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:12,299 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:12,903 - utils.text_chunker - INFO - Starting text chunking...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking text: 100%|██████████| 24/24 [00:00<00:00, 498333.15it/s]\n",
      "2025-04-30 11:54:12,913 - utils.text_chunker - INFO - Text successfully split into 24 chunks.\n",
      "2025-04-30 11:54:12,914 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:13,547 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:14,177 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:14,840 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:15,512 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:16,356 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:17,076 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:17,651 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:18,351 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:19,072 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:19,662 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:20,243 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:20,928 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:21,728 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:22,354 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:22,940 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:23,584 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:24,268 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:24,853 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:25,429 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:26,126 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:26,768 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:27,435 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:28,188 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:28,803 - utils.text_chunker - INFO - Starting text chunking...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking text: 100%|██████████| 2/2 [00:00<00:00, 33689.19it/s]\n",
      "2025-04-30 11:54:28,808 - utils.text_chunker - INFO - Text successfully split into 2 chunks.\n",
      "2025-04-30 11:54:28,808 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:29,518 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 11:54:30,555 - SummarizationPipeline - INFO - Initializing pipeline with model google-t5/t5-base\n",
      "2025-04-30 11:54:30,557 - SummarizationPipeline - INFO - Loading tokenizer for google-t5/t5-base\n",
      "2025-04-30 11:54:31,051 - SummarizationPipeline - INFO - Loading model for google-t5/t5-base (8bit=False, device=cuda)\n",
      "2025-04-30 11:54:32,091 - SummarizationPipeline - INFO - Model and tokenizer loaded successfully. Model max length: 1000000000000000019884624838656\n",
      "2025-04-30 11:54:32,092 - SummarizationPipeline - INFO - Using prefix: summarize: \n",
      "2025-04-30 11:54:32,099 - SummarizationPipeline - WARNING - Model vocab size (32100) and tokenizer vocab size (32128) mismatch. Resizing model embeddings.\n",
      "2025-04-30 11:54:32,103 - utils.text_chunker - INFO - Initialized TextChunker with chunk_size=1024, chunk_overlap=102, prefix=\"\"\n",
      "2025-04-30 11:54:32,111 - utils.text_chunker - INFO - Starting text chunking...\n",
      "Chunking text: 100%|██████████| 13/13 [00:00<00:00, 218979.73it/s]\n",
      "2025-04-30 11:54:32,118 - utils.text_chunker - INFO - Text successfully split into 13 chunks.\n",
      "2025-04-30 11:54:32,119 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:32,941 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:33,610 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:34,248 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:34,726 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:35,343 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:36,188 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:36,812 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:37,516 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:38,120 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:38,678 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:39,485 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:40,089 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:40,672 - utils.text_chunker - INFO - Starting text chunking...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking text: 100%|██████████| 16/16 [00:00<00:00, 244032.23it/s]\n",
      "2025-04-30 11:54:40,682 - utils.text_chunker - INFO - Text successfully split into 16 chunks.\n",
      "2025-04-30 11:54:40,682 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:41,396 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:42,165 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:43,028 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:43,869 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:44,494 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:45,130 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:45,597 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:46,277 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:47,261 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:48,087 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:48,793 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:49,656 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:50,348 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:51,085 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:51,602 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:52,286 - utils.text_chunker - INFO - Starting text chunking...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking text: 100%|██████████| 24/24 [00:00<00:00, 347114.81it/s]\n",
      "2025-04-30 11:54:52,298 - utils.text_chunker - INFO - Text successfully split into 24 chunks.\n",
      "2025-04-30 11:54:52,298 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:52,910 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:53,509 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:54,154 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:54,799 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:55,595 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:56,163 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:56,742 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:57,551 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:58,637 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:54:59,584 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:55:00,242 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:55:01,014 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:55:01,660 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:55:02,170 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:55:02,750 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:55:03,328 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:55:04,263 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:55:04,911 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:55:05,545 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:55:06,271 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:55:07,107 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:55:07,799 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:55:08,424 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:55:09,103 - utils.text_chunker - INFO - Starting text chunking...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking text: 100%|██████████| 2/2 [00:00<00:00, 34952.53it/s]\n",
      "2025-04-30 11:55:09,107 - utils.text_chunker - INFO - Text successfully split into 2 chunks.\n",
      "2025-04-30 11:55:09,108 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:55:09,860 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 11:55:11,151 - SummarizationPipeline - INFO - Initializing pipeline with model google/pegasus-x-large\n",
      "2025-04-30 11:55:11,152 - SummarizationPipeline - INFO - Loading tokenizer for google/pegasus-x-large\n",
      "2025-04-30 11:55:11,152 - SummarizationPipeline - INFO - Using specialized PegasusTokenizer\n",
      "You are using a model of type pegasus_x to instantiate a model of type pegasus. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-x-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.decoder.layers.0.encoder_attn.k_proj.bias', 'model.decoder.layers.0.encoder_attn.out_proj.bias', 'model.decoder.layers.0.encoder_attn.q_proj.bias', 'model.decoder.layers.0.encoder_attn.v_proj.bias', 'model.decoder.layers.0.self_attn.k_proj.bias', 'model.decoder.layers.0.self_attn.out_proj.bias', 'model.decoder.layers.0.self_attn.q_proj.bias', 'model.decoder.layers.0.self_attn.v_proj.bias', 'model.decoder.layers.1.encoder_attn.k_proj.bias', 'model.decoder.layers.1.encoder_attn.out_proj.bias', 'model.decoder.layers.1.encoder_attn.q_proj.bias', 'model.decoder.layers.1.encoder_attn.v_proj.bias', 'model.decoder.layers.1.self_attn.k_proj.bias', 'model.decoder.layers.1.self_attn.out_proj.bias', 'model.decoder.layers.1.self_attn.q_proj.bias', 'model.decoder.layers.1.self_attn.v_proj.bias', 'model.decoder.layers.10.encoder_attn.k_proj.bias', 'model.decoder.layers.10.encoder_attn.out_proj.bias', 'model.decoder.layers.10.encoder_attn.q_proj.bias', 'model.decoder.layers.10.encoder_attn.v_proj.bias', 'model.decoder.layers.10.self_attn.k_proj.bias', 'model.decoder.layers.10.self_attn.out_proj.bias', 'model.decoder.layers.10.self_attn.q_proj.bias', 'model.decoder.layers.10.self_attn.v_proj.bias', 'model.decoder.layers.11.encoder_attn.k_proj.bias', 'model.decoder.layers.11.encoder_attn.out_proj.bias', 'model.decoder.layers.11.encoder_attn.q_proj.bias', 'model.decoder.layers.11.encoder_attn.v_proj.bias', 'model.decoder.layers.11.self_attn.k_proj.bias', 'model.decoder.layers.11.self_attn.out_proj.bias', 'model.decoder.layers.11.self_attn.q_proj.bias', 'model.decoder.layers.11.self_attn.v_proj.bias', 'model.decoder.layers.12.encoder_attn.k_proj.bias', 'model.decoder.layers.12.encoder_attn.out_proj.bias', 'model.decoder.layers.12.encoder_attn.q_proj.bias', 'model.decoder.layers.12.encoder_attn.v_proj.bias', 'model.decoder.layers.12.self_attn.k_proj.bias', 'model.decoder.layers.12.self_attn.out_proj.bias', 'model.decoder.layers.12.self_attn.q_proj.bias', 'model.decoder.layers.12.self_attn.v_proj.bias', 'model.decoder.layers.13.encoder_attn.k_proj.bias', 'model.decoder.layers.13.encoder_attn.out_proj.bias', 'model.decoder.layers.13.encoder_attn.q_proj.bias', 'model.decoder.layers.13.encoder_attn.v_proj.bias', 'model.decoder.layers.13.self_attn.k_proj.bias', 'model.decoder.layers.13.self_attn.out_proj.bias', 'model.decoder.layers.13.self_attn.q_proj.bias', 'model.decoder.layers.13.self_attn.v_proj.bias', 'model.decoder.layers.14.encoder_attn.k_proj.bias', 'model.decoder.layers.14.encoder_attn.out_proj.bias', 'model.decoder.layers.14.encoder_attn.q_proj.bias', 'model.decoder.layers.14.encoder_attn.v_proj.bias', 'model.decoder.layers.14.self_attn.k_proj.bias', 'model.decoder.layers.14.self_attn.out_proj.bias', 'model.decoder.layers.14.self_attn.q_proj.bias', 'model.decoder.layers.14.self_attn.v_proj.bias', 'model.decoder.layers.15.encoder_attn.k_proj.bias', 'model.decoder.layers.15.encoder_attn.out_proj.bias', 'model.decoder.layers.15.encoder_attn.q_proj.bias', 'model.decoder.layers.15.encoder_attn.v_proj.bias', 'model.decoder.layers.15.self_attn.k_proj.bias', 'model.decoder.layers.15.self_attn.out_proj.bias', 'model.decoder.layers.15.self_attn.q_proj.bias', 'model.decoder.layers.15.self_attn.v_proj.bias', 'model.decoder.layers.2.encoder_attn.k_proj.bias', 'model.decoder.layers.2.encoder_attn.out_proj.bias', 'model.decoder.layers.2.encoder_attn.q_proj.bias', 'model.decoder.layers.2.encoder_attn.v_proj.bias', 'model.decoder.layers.2.self_attn.k_proj.bias', 'model.decoder.layers.2.self_attn.out_proj.bias', 'model.decoder.layers.2.self_attn.q_proj.bias', 'model.decoder.layers.2.self_attn.v_proj.bias', 'model.decoder.layers.3.encoder_attn.k_proj.bias', 'model.decoder.layers.3.encoder_attn.out_proj.bias', 'model.decoder.layers.3.encoder_attn.q_proj.bias', 'model.decoder.layers.3.encoder_attn.v_proj.bias', 'model.decoder.layers.3.self_attn.k_proj.bias', 'model.decoder.layers.3.self_attn.out_proj.bias', 'model.decoder.layers.3.self_attn.q_proj.bias', 'model.decoder.layers.3.self_attn.v_proj.bias', 'model.decoder.layers.4.encoder_attn.k_proj.bias', 'model.decoder.layers.4.encoder_attn.out_proj.bias', 'model.decoder.layers.4.encoder_attn.q_proj.bias', 'model.decoder.layers.4.encoder_attn.v_proj.bias', 'model.decoder.layers.4.self_attn.k_proj.bias', 'model.decoder.layers.4.self_attn.out_proj.bias', 'model.decoder.layers.4.self_attn.q_proj.bias', 'model.decoder.layers.4.self_attn.v_proj.bias', 'model.decoder.layers.5.encoder_attn.k_proj.bias', 'model.decoder.layers.5.encoder_attn.out_proj.bias', 'model.decoder.layers.5.encoder_attn.q_proj.bias', 'model.decoder.layers.5.encoder_attn.v_proj.bias', 'model.decoder.layers.5.self_attn.k_proj.bias', 'model.decoder.layers.5.self_attn.out_proj.bias', 'model.decoder.layers.5.self_attn.q_proj.bias', 'model.decoder.layers.5.self_attn.v_proj.bias', 'model.decoder.layers.6.encoder_attn.k_proj.bias', 'model.decoder.layers.6.encoder_attn.out_proj.bias', 'model.decoder.layers.6.encoder_attn.q_proj.bias', 'model.decoder.layers.6.encoder_attn.v_proj.bias', 'model.decoder.layers.6.self_attn.k_proj.bias', 'model.decoder.layers.6.self_attn.out_proj.bias', 'model.decoder.layers.6.self_attn.q_proj.bias', 'model.decoder.layers.6.self_attn.v_proj.bias', 'model.decoder.layers.7.encoder_attn.k_proj.bias', 'model.decoder.layers.7.encoder_attn.out_proj.bias', 'model.decoder.layers.7.encoder_attn.q_proj.bias', 'model.decoder.layers.7.encoder_attn.v_proj.bias', 'model.decoder.layers.7.self_attn.k_proj.bias', 'model.decoder.layers.7.self_attn.out_proj.bias', 'model.decoder.layers.7.self_attn.q_proj.bias', 'model.decoder.layers.7.self_attn.v_proj.bias', 'model.decoder.layers.8.encoder_attn.k_proj.bias', 'model.decoder.layers.8.encoder_attn.out_proj.bias', 'model.decoder.layers.8.encoder_attn.q_proj.bias', 'model.decoder.layers.8.encoder_attn.v_proj.bias', 'model.decoder.layers.8.self_attn.k_proj.bias', 'model.decoder.layers.8.self_attn.out_proj.bias', 'model.decoder.layers.8.self_attn.q_proj.bias', 'model.decoder.layers.8.self_attn.v_proj.bias', 'model.decoder.layers.9.encoder_attn.k_proj.bias', 'model.decoder.layers.9.encoder_attn.out_proj.bias', 'model.decoder.layers.9.encoder_attn.q_proj.bias', 'model.decoder.layers.9.encoder_attn.v_proj.bias', 'model.decoder.layers.9.self_attn.k_proj.bias', 'model.decoder.layers.9.self_attn.out_proj.bias', 'model.decoder.layers.9.self_attn.q_proj.bias', 'model.decoder.layers.9.self_attn.v_proj.bias', 'model.encoder.embed_positions.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.12.self_attn.k_proj.bias', 'model.encoder.layers.12.self_attn.out_proj.bias', 'model.encoder.layers.12.self_attn.q_proj.bias', 'model.encoder.layers.12.self_attn.v_proj.bias', 'model.encoder.layers.13.self_attn.k_proj.bias', 'model.encoder.layers.13.self_attn.out_proj.bias', 'model.encoder.layers.13.self_attn.q_proj.bias', 'model.encoder.layers.13.self_attn.v_proj.bias', 'model.encoder.layers.14.self_attn.k_proj.bias', 'model.encoder.layers.14.self_attn.out_proj.bias', 'model.encoder.layers.14.self_attn.q_proj.bias', 'model.encoder.layers.14.self_attn.v_proj.bias', 'model.encoder.layers.15.self_attn.k_proj.bias', 'model.encoder.layers.15.self_attn.out_proj.bias', 'model.encoder.layers.15.self_attn.q_proj.bias', 'model.encoder.layers.15.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-04-30 11:55:59,808 - SummarizationPipeline - INFO - Model and tokenizer loaded successfully. Model max length: 1024\n",
      "2025-04-30 11:55:59,809 - SummarizationPipeline - INFO - Using prefix: \n",
      "2025-04-30 11:55:59,809 - SummarizationPipeline - INFO - Model and tokenizer vocab sizes match. No resizing needed.\n",
      "2025-04-30 11:55:59,809 - utils.text_chunker - INFO - Initialized TextChunker with chunk_size=1024, chunk_overlap=102, prefix=\"\"\n",
      "2025-04-30 11:55:59,827 - utils.text_chunker - INFO - Starting text chunking...\n",
      "Chunking text: 100%|██████████| 13/13 [00:00<00:00, 271273.39it/s]\n",
      "2025-04-30 11:55:59,835 - utils.text_chunker - INFO - Text successfully split into 13 chunks.\n",
      "2025-04-30 11:55:59,835 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:56:02,223 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:56:04,850 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:56:06,218 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:56:08,339 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:56:10,246 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:56:11,564 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:56:13,487 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:56:15,344 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:56:16,654 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:56:18,508 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:56:20,687 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:56:22,534 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1097 > 1024). Running this sequence through the model will result in indexing errors\n",
      "2025-04-30 11:56:24,095 - utils.text_chunker - INFO - Starting text chunking...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking text: 100%|██████████| 2/2 [00:00<00:00, 46345.90it/s]\n",
      "2025-04-30 11:56:24,098 - utils.text_chunker - INFO - Text successfully split into 2 chunks.\n",
      "2025-04-30 11:56:24,098 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:56:25,728 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:56:27,224 - utils.text_chunker - INFO - Starting text chunking...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking text: 100%|██████████| 16/16 [00:00<00:00, 280790.23it/s]\n",
      "2025-04-30 11:56:27,232 - utils.text_chunker - INFO - Text successfully split into 16 chunks.\n",
      "2025-04-30 11:56:27,233 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:56:28,937 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:56:31,207 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:56:32,815 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:56:34,384 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:56:36,379 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:56:38,117 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:56:39,675 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:56:42,243 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:56:44,100 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:56:46,527 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:56:48,034 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:56:49,887 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:56:52,434 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:56:54,187 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:56:55,677 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:56:58,192 - utils.text_chunker - INFO - Starting text chunking...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking text: 100%|██████████| 2/2 [00:00<00:00, 47662.55it/s]\n",
      "2025-04-30 11:56:58,196 - utils.text_chunker - INFO - Text successfully split into 2 chunks.\n",
      "2025-04-30 11:56:58,196 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:56:59,940 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:57:02,545 - utils.text_chunker - INFO - Starting text chunking...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking text: 100%|██████████| 24/24 [00:00<00:00, 387166.52it/s]\n",
      "2025-04-30 11:57:02,556 - utils.text_chunker - INFO - Text successfully split into 24 chunks.\n",
      "2025-04-30 11:57:02,556 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:57:04,519 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:57:06,159 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:57:07,515 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:57:08,743 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:57:11,486 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:57:13,788 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:57:16,495 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:57:20,632 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:57:24,996 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:57:26,471 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:57:28,193 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:57:32,468 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:57:34,083 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:57:35,583 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:57:36,880 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:57:38,466 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:57:40,123 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:57:45,085 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:57:47,337 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:57:49,681 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:57:52,093 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:57:53,475 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:57:55,107 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:57:56,817 - utils.text_chunker - INFO - Starting text chunking...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking text: 100%|██████████| 3/3 [00:00<00:00, 69136.88it/s]\n",
      "2025-04-30 11:57:56,822 - utils.text_chunker - INFO - Text successfully split into 3 chunks.\n",
      "2025-04-30 11:57:56,822 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:57:58,460 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:00,441 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 11:58:03,864 - SummarizationPipeline - INFO - Initializing pipeline with model human-centered-summarization/financial-summarization-pegasus\n",
      "2025-04-30 11:58:03,864 - SummarizationPipeline - INFO - Loading tokenizer for human-centered-summarization/financial-summarization-pegasus\n",
      "2025-04-30 11:58:03,865 - SummarizationPipeline - INFO - Using specialized PegasusTokenizer\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at human-centered-summarization/financial-summarization-pegasus and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-04-30 11:58:07,114 - SummarizationPipeline - INFO - Model and tokenizer loaded successfully. Model max length: 512\n",
      "2025-04-30 11:58:07,115 - SummarizationPipeline - INFO - Using prefix: \n",
      "2025-04-30 11:58:07,115 - SummarizationPipeline - INFO - Model and tokenizer vocab sizes match. No resizing needed.\n",
      "2025-04-30 11:58:07,116 - utils.text_chunker - INFO - Initialized TextChunker with chunk_size=512, chunk_overlap=51, prefix=\"\"\n",
      "2025-04-30 11:58:07,129 - utils.text_chunker - INFO - Starting text chunking...\n",
      "Chunking text: 100%|██████████| 26/26 [00:00<00:00, 507218.16it/s]\n",
      "2025-04-30 11:58:07,136 - utils.text_chunker - INFO - Text successfully split into 26 chunks.\n",
      "2025-04-30 11:58:07,136 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:07,906 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:08,807 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:09,605 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:10,370 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:11,197 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:11,961 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:12,733 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:13,563 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:14,422 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:15,188 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:15,972 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:16,754 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:17,532 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:18,298 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:19,169 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:19,969 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:20,737 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:21,524 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:22,294 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:23,063 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:23,898 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:24,730 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:25,504 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:26,482 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:27,293 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1334 > 512). Running this sequence through the model will result in indexing errors\n",
      "2025-04-30 11:58:28,093 - utils.text_chunker - INFO - Starting text chunking...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking text: 100%|██████████| 3/3 [00:00<00:00, 68385.39it/s]\n",
      "2025-04-30 11:58:28,096 - utils.text_chunker - INFO - Text successfully split into 3 chunks.\n",
      "2025-04-30 11:58:28,096 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:28,858 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:29,688 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:30,514 - utils.text_chunker - INFO - Starting text chunking...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking text: 100%|██████████| 33/33 [00:00<00:00, 574323.78it/s]\n",
      "2025-04-30 11:58:30,521 - utils.text_chunker - INFO - Text successfully split into 33 chunks.\n",
      "2025-04-30 11:58:30,521 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:31,290 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:32,102 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:32,874 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:33,706 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:34,485 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:35,261 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:36,052 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:36,844 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:37,708 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:38,458 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:39,235 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:40,156 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:41,019 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:41,782 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:42,549 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:43,383 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:44,174 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:44,940 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:45,702 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:46,493 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:47,255 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:48,048 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:48,829 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:49,603 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:50,366 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:51,245 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:52,040 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:52,922 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:53,689 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:54,464 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:55,233 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:56,099 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:56,883 - utils.text_chunker - INFO - Starting text chunking...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking text: 100%|██████████| 4/4 [00:00<00:00, 73262.95it/s]\n",
      "2025-04-30 11:58:56,886 - utils.text_chunker - INFO - Text successfully split into 4 chunks.\n",
      "2025-04-30 11:58:56,886 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:57,830 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:58,594 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:58:59,384 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:00,151 - utils.text_chunker - INFO - Starting text chunking...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking text: 100%|██████████| 47/47 [00:00<00:00, 684487.11it/s]\n",
      "2025-04-30 11:59:00,161 - utils.text_chunker - INFO - Text successfully split into 47 chunks.\n",
      "2025-04-30 11:59:00,161 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:00,927 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:01,736 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:02,522 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:03,316 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:04,159 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:04,983 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:05,744 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:06,511 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:07,334 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:08,185 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:08,950 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:09,728 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:10,492 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:11,258 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:12,071 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:12,834 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:13,732 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:14,539 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:15,364 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:16,201 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:16,994 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:17,759 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:18,649 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:19,447 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:20,312 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:21,122 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:21,894 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:22,911 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:23,723 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:24,491 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:25,299 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:26,045 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:26,811 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:27,580 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:28,345 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:29,169 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:29,950 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:30,788 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:31,597 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:32,362 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:33,173 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:33,953 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:34,716 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:35,617 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:36,398 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:37,253 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:38,040 - utils.text_chunker - INFO - Starting text chunking...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking text: 100%|██████████| 6/6 [00:00<00:00, 135300.13it/s]\n",
      "2025-04-30 11:59:38,044 - utils.text_chunker - INFO - Text successfully split into 6 chunks.\n",
      "2025-04-30 11:59:38,044 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:39,020 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:39,816 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:40,705 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:41,472 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n",
      "2025-04-30 11:59:42,269 - SummarizationPipeline - INFO - Generating summary (max_new_tokens=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 2\n"
     ]
    }
   ],
   "source": [
    "checkpoints = ['facebook/bart-large-cnn', 'google-t5/t5-base', 'google/pegasus-x-large', 'human-centered-summarization/financial-summarization-pegasus']\n",
    "\n",
    "logging_config: LoggingConfig = LoggingConfig()\n",
    "\n",
    "summaries = []\n",
    "\n",
    "for checkpoint in checkpoints:\n",
    "\n",
    "    model_config: ModelConfig = ModelConfig(\n",
    "        model_name_or_path=checkpoint, device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    )\n",
    "\n",
    "    pipeline = SummarizationPipeline(model_config=model_config, logging_config=logging_config)\n",
    "    \n",
    "    tokenizer = pipeline.get_tokenizer()\n",
    "    chunker = TextChunker(tokenizer)\n",
    "\n",
    "    try:\n",
    "        for transcript in tqdm(transcripts):\n",
    "            chunks = chunker.chunk_text(transcript)\n",
    "            chunk_summaries = [pipeline.summarize(chunk) for chunk in chunks]\n",
    "            combined_summary = \" \".join(chunk_summaries)\n",
    "            max_rounds = 5\n",
    "            round_count = 0\n",
    "            final_summary = \"\"\n",
    "            while round_count < max_rounds:\n",
    "                print(f'round {round_count+1}')\n",
    "                input_ids = tokenizer(combined_summary, return_tensors='pt', truncation=False)['input_ids']\n",
    "                if input_ids.shape[1] <= min(1024, pipeline.model_max_length):\n",
    "                    final_summary = combined_summary\n",
    "                    break\n",
    "                re_chunks = chunker.chunk_text(combined_summary)\n",
    "                re_chunk_summaries = [pipeline.summarize(chunk) for chunk in re_chunks]\n",
    "                combined_summary = \" \".join(re_chunk_summaries)\n",
    "                round_count += 1\n",
    "            else:\n",
    "                final_summary = combined_summary\n",
    "            summaries.append(final_summary)\n",
    "    finally:\n",
    "        del pipeline\n",
    "        del model_config\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['JPMorgan\\'s 21st Annual Technology and Automotive Investor Forum is being held at the Consumer Electronics Show. Colette Kress, Executive Vice President and Chief Financial Officer at NVIDIA, will be the first to present. Kress: We are focused on bringing to market a large lineup of new products, particularly in our data center. Oren: FoxConn has decided to join and has adopted our Oren architecture for compute in the many NEVs, not just the NEVs that we are already a part of, but any type of NEV, new energy types of cars across the world. Omniverse has, of course, now been adopted by Mercedes in their next-generation factories similar to some of the other automotive companies. On gaming, in particular, Q2 and Q3 combined, you\\'re shipping about 30% below end demand consumption of around $5 billion. We believe that this is just through a combination of the holiday season and a direct focus on making sure working with our AIC partners and our OEMs to move that inventory. Hewlett-Packard: We\\'re in the right position with our architecture to both improve performance of nearly 5x or more from the last generation. We\\'re all set up for all of these right things as the market continues to grow. We expect our Grace CPU to help influence the connectivity of working with our GPU.  NVIDIA CEO: \"We have in our installed base, more than 40% focused on ray tracing\" \"We\\'re on a great trajectory there to see that take off more\" \"I believe the software, services and ratable type of revenue streams, I think, is running sort of low sort of $100 million run rate\"  NVIDIA AI enterprise is a core you should think about as the operating system of AI available for our enterprises. Omniverse really fuels, what we will see as the future of 3D Internet and folks really working not only on those 3 dimensions, but what you may see in terms of digital twins. Colette Kress, Executive Vice President and Chief Financial Officer at NVIDIA. Kress: \"We are continuing to work to try and keep on a sequential basis about as flat as we can in this environment\" \"Our business model and our structure as a company allows quite a bit of efficiency,\" she said. Colette: We\\'re here for the consumer folks, both here for gaming as well as for auto. Talked about the 4070 Ti coming to market, and we are also bringing our 40 Series to notebooks. We have more than 170 different notebooks, probably able to see throughout the floor at the convention center. We\\'re not just focused on a singular GPU type of platform. We\\'re really focused on the data center computing as a whole. The environment couldn\\'t be more set up right now as folks have really understood that the ending of Moore\\'s Law is now really here. We are continuing a growth trajectory on our Automotive business. The company is focused on the next-generation Hopper architecture. The Grace CPU is expected to help influence the connectivity of working with our GPU. We\\'re seeing 3 very important areas in AI that were probably in their infancy or did not exist in the last 3 years or more. Executives: We\\'re on track for sampling in the first half, and we\\'ll bring this to market in the second half of the year. Despite a challenging consumer market, demand for the new Ada 40 Series gaming products seems quite good. 4080 GPU card pricing is marked up about 10%, 15% relative to MSRP. The company has an $11 billion design win pipeline that is expected to unfold over the next few years. The business is running -- I believe the software, services and ratable type of revenue streams, I think, is running sort of low sort of $100 million annualized run rate today. Executives: You\\'re going to see different stages, but we\\'ve already probably passed an inflection point that we\\'re seeing the growth of Orin with our NEVs. The next phase, we\\'ll be coming with our design wins both on robotaxis and bringing some of the early AV to market.',\n",
       " 'Harlan Surlan is an analyst for the semiconductor firm NVIDIA. He says he\\'s already seen a pickup in the use of generative AI in health care. Kimberly Powell, Vice President of Healthcare at NVIDIA, talks about the company\\'s worldwide health care business. The data center business for NVIDIA has grown at a 70% CAGR over the past 3 years.  NVIDIA CEO: \"ChatGPT is just -- it\\'s an amazing technology, and it\\'s just about what do you want to use it for?\" \"I think the industry and the number of therapies and the personalization of medicine, there\\'s room for everybody, if it were up to me\"',\n",
       " ' NVIDIA vGPU software delivers performance that is distinguishable from the physical PC. HFAae is consolidating its hardware to make sure all of its users have a better experience. VMWare virtualization can be used to get back online quickly in the event of a disaster. The company is offering a free 90-day trial of the technology to all attendees. HFA is a multi-disciplined firm with 4 offices across 2 countries, but a lot of remote workers. HFA started out in 1990 as an architecture only firm here in Bentonville, Arkansas. The firm has grown in other sectors such as retail, C-store, restaurants, commercial real estate development.',\n",
       " 'cnn\\'s colette kress is the first to present at the CES investor conference. she\\'s the executive vice president and chief financial officer at NVIDIA. kress: \"we are really pleased in terms of what we\\'re seeing\" if we hit our guidance for calendar \\'22, revenue for calendar \\'22 is flattish. we\\'re bringing our Grace CPU to market and our networking business to market. we\\'re also bringing our $11 billion pipeline in automotive. gaming is a very important area for the company to focus on. the team is tracking towards 40% year-over-year growth in fiscal \\'23. the team is forecasting cloud CapEx spending still up high single digits this year. generative AI is a full lineup across the board. Grace CPU is focused on very large models focusing on AI. Grace will be available in the first half of the year. ADA 40 series is a new generation of RTX GPUs. the company is bringing a new generation to market every 2-plus years. ADA is also focusing on a new kind of supercomputing. team has $11 billion design win pipeline in automotive, but is driving 100% year-over-year growth. \"we\\'re seeing the future of 3D Internet and folks really working on those 3 dimensions,\" says steve mcclaren. team is also focusing on bringing some of the early AV to market. colette Kress is executive vice president and chief financial officer at NVIDIA. she\\'s the first to present at the CES investor conference. the company is driving many of the trends that you\\'re going to hear about today. cnn\\'s colette scott is the chief executive officer of a gaming company. she says the company is focusing on the data center and the automotive business. scott: \"we\\'re really focused on the data center computing as a whole\" DLSS is a third generation of ray tracing technology. the team is working on a $11 billion pipeline in automotive. the team is tracking towards 40% year-over-year growth in fiscal \\'23. cloud CapEx spending is forecast to grow by 2x this year. the company is focusing on accelerating compute and generative AI. the company is also focusing on a new generation of Grace CPUs. despite a challenging consumer market, demand for the new 40 series seems good. 4090 sold out quite quickly, and 4080 GPU card pricing is marked up about 10%, 15%. \"we\\'re on a great trajectory there to see that take off more,\" says steve mcdonald. team has $11 billion design win pipeline in automotive. team is monetizing its software platforms and is selling it separately. team is also focusing on a new generation of nvidia\\'s i7 processors. orin is a company focused on bringing electric cars to market. the company is still in the hardware infrastructure that\\'s driving our revenue. but we\\'re also bringing some of the early AV to market.',\n",
       " 'analyst: \"we\\'re rapidly becoming the de facto standard for surgical robotics platforms\" \"as sensors evolve, so do the computing platforms and the applications in the health care delivery\" \"we\\'re bringing the power of the ecosystems, of imaging, genomics, life sciences\" nvidia is building an end-to-end computing platform to serve the medical device market. nvidia\\'s 2022 genomics year was an absolute breakout year for the company. nvidia is partnering across the genomics industry from new sequencers to cloud services. generative AI is seeing broad applicability across life sciences and drug discovery. a team of scientists at meta, roast lab, barsele lab and alabama are accelerating 96 optical genome mapping workflows. the team is bringing the technology to the public cloud and in the labs at the uw. a collaboration between NVIDIA and instadeep has resulted in a state-of-the-art genomics language model. the model can generalize across many tasks, and can be used to generate protein libraries. the model can be used to help identify and predict the toxicity of drugs. a new generation of protein-based models is enabling generative AI and biology. the first protein generated had 51 mutations, which is about 85% similar to the original. the company dedicated itself to artificial intelligence 10 years ago. accelerated compute spending within health care continues to grow at an extremely rapid rate. we believe that we could be the next $1 billion industry for NVIDIA. a lot of the value we can provide is in generative AI and large language models. a huge amount of computing is happening in pharmaceuticals. a huge part of that is coming from simulation. a huge portion of that is coming from generative AI. the team just rolled out its next-generation compute acceleration platform, the H100. it\\'s based on the 4-nanometer, leading-edge manufacturing technology. the team is looking at how to make a better, more efficient, more efficient machine. asian-american semiconductor analyst presents at the 41st annual healthcare conference. he says the company is a leader in accelerated computing semiconductors. he says the company is a time machine, but it\\'s also a full-stack computing platform company. asian-americans are the next generation of health care technology, he says. nvidia is building an end-to-end computing platform to serve the medical device market. the company is partnering across the genomics industry, from new sequencers to cloud services. the company is bringing the nvidia platform to the market, which is a scalable real-time AI platform. a team of scientists has partnered with u.s. universities to accelerate genomic sequencing. the team is using generative AI to build language models for genomic sequencing. the team is also working on a new ios platform that will be available in 2022. a collaboration between NVIDIA and instadeep has resulted in a state-of-the-art genomics language model. the model is able to generalize across many tasks, and is able to generate proteins. the model is a generative AI model for protein engineering called ProT-VAE. a new generation of protein-based models is enabling generative AI and biology. the first protein generated had 51 mutations, which is about 85% similar to the original. a new generation of models is enabling a new generation of proteins to be created. data center business for NVIDIA has grown at a 70% CAGR over the past 3 years. underneath that, accelerated compute spending within health care continues to grow at an extremely rapid rate. we believe that we could be the next $1 billion industry for NVIDIA. a huge chunk of the $1 billion in revenue is coming from generative AI. a huge chunk of that is coming from medical devices. a huge chunk of that is coming from simulation. the team just rolled out its next-generation compute acceleration platform, the H100. it\\'s based on the 4-nanometer, leading-edge manufacturing technology. the team is looking at how to make a better, more efficient, more efficient machine.',\n",
       " \"cnn's john sutter talks about the benefits of virtualization in the data center. vGPUs are a powerful way to make virtualization more efficient. sutter: virtualization is a great way to reduce e-waste and sustainability. hfa-ae: vGPUs are a great way to get a better understanding of your users. hfa: we're able to get so much more life out of the products and hardware. hfa: vGPUs are a great example of how technology helps solve many of these issues.\",\n",
       " \"It's been a tradition, 9 years, to have the NVIDIA team, specifically Colette, be the first to present at our investor conference because the team is driving many of the trends that you're going to hear about today, right, artificial intelligence, compute acceleration, next-generation compute platforms in automotive, in gaming and IoT. Omniverse really fuels what we will see as the future of 3D Internet and folks really working not only on those 3 dimensionals but what you may see in terms of digital twins and work in that piece. For many years, we have put software in both the ecosystem as well as in our products that enables customers to take out of a box and begin their work that they need because we've enabled a lot of that software.\",\n",
       " \"So the ability to program the chips, but also be able to introduce acceleration libraries, applications and purpose-built computing platforms, whether that's large-scale data centers for artificial intelligence or embedded supercomputers for the medical devices industry And then to help close the gap between research and clinical deployment, we announced last year, NVIDIA HolliScan is meant to be a commercial off-the-shelf more general purpose computing platform for these applications to live, so that not every medical device needs to reinvent their computing platform every time a new sensor technology comes to market. MONAI for imaging, Parabricks for genomics, BioNeMo for drug discovery are helping the industry harness the computation and massive accelerate R&D and the workflows that drive health care and life sciences. As a follow-on from that perspective around this potentially becoming a $1 billion industry for the data center sort of value proposition that NVIDIA can underpin, which of the traditional health care stakeholders need to enable or buy into the value chain of your end customers?\",\n",
       " 'And that acquisition really accelerated the adoption of vGPU virtualization because we were trying to figure out how we would get these users on systems and hit a way that didn\\'t involve us having to take servers all the way to that location, manage those and then the costs associated with them and the long-term administration of those systems. This has a lot of different applications.\" And so initially, we were thinking of using it as a way just for when we\\'re on site with a client to access for files and better show it versus having to load up a bunch of different files on a laptop and risk missing something where we could actually access our file servers in real time without having to set up a VPN connection. With that density though and reducing the amount of hardware required in your data center, it really helps reduce your footprint and sustainability is something I\\'m also pretty passionate about and making sure that we aren\\'t generating a lot of e-waste and virtualization is a great way to do that because what it allows us to do is instead of just we\\'re upgrading this set of users, time to take those old machines and throw them in the garbage or never look at them again in a supply closet until they get so big that they\\'re falling out of the closet.',\n",
       " \"Executive Vice President and Chief Financial Officer, Colette Kress, will speak at JPMorgan's 21st Annual Technology and Automotive Investor Forum. Here is the live webcast of Nvidia's conference call in Las Vegas, Nevada on Wednesday, January 10 Call with analysts after market close on Monday, January 7, 2019. Nvidia to report fourth-quarter and full-year results on Tuesday, Jan. 15, at 22:35 local time on the West Coast Here's a rundown of some of the highlights in the world of gaming Analysts: Is there any word on Grace pre-integrated? Jensen: We're on a great trajectory to see more of our 4090, 4080 come to market Analysts: You mentioned at the special address that 40% of your installed base has adopted GPUs.\",\n",
       " \"Company to present at JPMorgan's 41st Annual Healthcare Conference. Kimberly Powell, Vice President of Healthcare at AMD, will speak on Tuesday, June 5 Analysts: What do you see for the future of artificial intelligence Analysts: Can you give us some examples of what you've been up to in the last year Intel CEO talks about the company's end-to-end computing platform for health care. Here's what he had to say in response to a question about the impact of artificial intelligence Here are some of the highlights from the conference call: CEO: I want to go quickly next to a modality that is super, super exciting. Here's what he had to say in response to a question about the impact of artificial intelligence on the health care industry: Here are some highlights from the call: CEO says ChatGPT could be in pharma use cases within 5 months. Analysts ask about split between TechBio and Pharma, with CEO saying: “Within the next 5 months” Some of the highlights from the call can be found in its entirety:\",\n",
       " \"Panel will look at 4 major AECO industry trends and their impact on the GPU market. Trademarkia is aTrademarkiaTrademarkia firm with offices in the U.S., Canada, Mexico, and moreTrademarkia attendees: Tell us a little bit about your firm and what it is like to be the only firm in the world Hfa has been working closer with its brand team to push its to a whole new level. Users now running HFA on Xen, up from 257 in 2020 Attendees: We had more people on VDIs, and so one of the biggest things VDI is here to stay, but what is it doing for the AEC market? Poll shows almost 60% are not using VDI AEC firms can take advantage of VDI performance, ease-of-use, and cost-savings Here are some highlights from the call: Poll question: are you satisfied with your current experience? AEC firms can take advantage of VDI performance, ease-of-use, and cost-savings Here's a look at some of the top VDI use cases in the industry VDI World interview with Autodesk VP of engineering. Hfa-ae senior vice president of architecture, engineering, and planning: What was the need for consolidation and what did It's exciting to see what we can do with digital twin in this Q&A During a Q&A session at E3 2018, AMD's senior VP of marketing spoke about the company's A100 and A40 cards: Attendees: So, let's talk about the A100, because it's a lot more for AI\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
